# ASA Ã— LFM2.5-1.2B-Instruct

**Training-Free Tool-Calling Enhancement via Activation Steering**

[ðŸ‡°ðŸ‡· í•œêµ­ì–´ ë²„ì „](README_KR.md)

---

## Overview

This project applies the **ASA (Activation Steering Adapter)** technique to [LiquidAI/LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct) â€” a 1.17B parameter hybrid model with 10 LIV convolution blocks + 6 GQA attention blocks.

ASA enhances tool-calling capabilities at inference time **without any model retraining**, using only ~20KB of additional assets.

> ðŸ“„ Paper: [ASA: Training-Free Representation Engineering for Tool-Calling Agents](https://arxiv.org/abs/2602.04935)

## How ASA Works

```mermaid
graph LR
    A[Input] --> B[Model Forward Pass]
    B --> C{Layer L*}
    C --> D[Router: Classify Domain]
    D --> E[Probe: Tool Intent Score]
    E --> F{Ternary Gate}
    F -->|p â‰¥ Ï„: +1| G[Inject +Î±v â†’ Promote Tool Call]
    F -->|p â‰¤ 1-Ï„: -1| H[Inject -Î±v â†’ Suppress Tool Call]
    F -->|else: 0| I[No Intervention]
    G --> J[Continue Generation]
    H --> J
    I --> J
```

**Key insight:** LLMs often *internally represent* tool-use intent but fail to act on it ("Lazy Agent" phenomenon). ASA bridges this representation-behavior gap by nudging the hidden state toward the tool-calling direction.

## Project Structure

```
Liquid-ASA/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ tools.json           # 4 tool definitions
â”‚   â”œâ”€â”€ cal_data.json         # 320 calibration samples (vector extraction)
â”‚   â”œâ”€â”€ train_data.json       # 320 training samples (router/probe)
â”‚   â”œâ”€â”€ valid_data.json       # 160 validation samples (hyperparameter tuning)
â”‚   â””â”€â”€ test_data.json        # 320 test samples (evaluation)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ asa_lfm25_colab.py    # Complete pipeline (Colab T4 compatible)
â”œâ”€â”€ outputs/                  # Generated assets, plots, checkpoints
â”œâ”€â”€ ASA_LFM2.5_Implementation_Plan.md
â”œâ”€â”€ README.md                 # English
â”œâ”€â”€ README_KR.md              # í•œêµ­ì–´
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

## Quick Start

### Option 1: Google Colab (Recommended)

1. Open [Google Colab](https://colab.research.google.com/)
2. Clone and run:

```python
!git clone https://github.com/gyunggyung/Liquid-ASA.git
%cd Liquid-ASA
```

3. Convert to notebook for **cell-by-cell execution**:

```python
!pip install -q jupytext
!jupytext --to ipynb notebooks/asa_lfm25_colab.py -o notebooks/asa_lfm25_colab.ipynb
```

4. Open `notebooks/asa_lfm25_colab.ipynb` and run each cell step by step.

### Option 2: Local Execution

```bash
git clone https://github.com/gyunggyung/Liquid-ASA.git
cd Liquid-ASA
pip install -r requirements.txt
python notebooks/asa_lfm25_colab.py
```

### Option 3: VS Code (Jupyter Extension)

Open `notebooks/asa_lfm25_colab.py` in VS Code with the Jupyter extension â€” all `# %%` cells are recognized automatically. Run each cell with **Shift+Enter**.

## Pipeline Stages

| Stage | Description | Output |
|-------|-------------|--------|
| 1. Setup | Install deps, detect GPU | â€” |
| 2. Data Loading | Load 1,120 samples, verify splits | Data stats |
| 3. Model Loading | Load LFM2.5 in float16 | Model info |
| 4. Hidden State Extraction | Forward hooks at all 16 layers | `{layer: (N, D)}` arrays |
| 5. Probe Sweep | Logistic regression AUC per layer | Best layer L* + plot |
| 6. Steering Vectors | Mean-diff vectors from CAL data | Per-domain + global vectors |
| 7. Router & Probes | Multi-class router + binary probes | Trained classifiers |
| 8. ASA Controller | `ASAController` with MoV + gate | Controller object |
| 9. Hyperparameter Tuning | Grid sweep Î±, Ï„, Î² on VALID | Optimal hyperparameters + plot |
| 10. Evaluation | Trigger metrics on TEST set | Precision/Recall/F1/FPR |
| 11. HF Upload | Save ~20KB assets to HuggingFace | Model card + assets |
| 12. Demo | Side-by-side baseline vs ASA | Interactive comparison |

> ðŸ’¡ Each stage is **checkpointed** â€” you can resume from any point. Intermediate results are saved to `outputs/`.

## Dataset

**1,120 manually crafted samples** across 4 domains Ã— 4 splits:

| Domain | Tools | Example (Tool) | Example (Non-Tool) |
|--------|-------|-----------------|---------------------|
| Math | `calculator` | "Calculate 15% tip on $85" | "Tell me about Euler" |
| Code | `python_interpreter` | "Sort the list [5,2,8,1]" | "What is recursion?" |
| Search | `web_search` | "Find latest AI news" | "What is photosynthesis?" |
| Translation | `translator` | "Translate 'hello' to Korean" | "History of English language" |

## LFM2.5 Adaptations

| Aspect | ASA Paper (Qwen/LLaMA) | This Project (LFM2.5) |
|--------|------------------------|----------------------|
| Architecture | Transformer-only | Hybrid LIV conv + GQA |
| Parameters | 1.5B / 8B | 1.17B |
| Layers | 28â€“32 | 16 (10 LIV + 6 GQA) |
| Tool tokens | `<functioncall>` | `<\|tool_call_start\|>` / `<\|tool_call_end\|>` |
| Tool format | JSON | Pythonic (Python list syntax) |
| Intervention | Single optimal layer | Probe sweep across all 16 |

## Expected Results

Based on the ASA paper's findings on similar-sized models (Qwen2.5-1.5B):

| Metric | Baseline | ASA (expected) |
|--------|----------|----------------|
| F1 Score | ~0.65 | ~0.80+ |
| FPR | ~0.15 | ~0.05 |
| Asset Size | â€” | ~20KB |
| Latency Overhead | â€” | <1ms |

## Citation

```bibtex
@article{asa2025,
  title={ASA: Training-Free Representation Engineering for Tool-Calling Agents},
  author={...},
  journal={arXiv preprint arXiv:2602.04935},
  year={2025}
}
```

## License

This project is licensed under the [Apache License 2.0](LICENSE).
