"""Generate ASA_Qwen15B_Reproduction.ipynb with Alpaca dynamic filtering (same as LFM2.5)."""
import json

cells = []

def md(src):
    cells.append({"cell_type":"markdown","metadata":{},"source": src if isinstance(src,list) else [src]})

def code(src):
    cells.append({"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source": src if isinstance(src,list) else [src]})

# ‚îÄ‚îÄ Header ‚îÄ‚îÄ
md([
    "# üß™ ASA Paper Reproduction: Qwen2.5-1.5B-Instruct\n",
    "**Reproducing Tables 1, 4-5 from [arXiv 2602.04935](https://arxiv.org/abs/2602.04935)**\n",
    "\n",
    "Target: L*=18, AUC=0.9996, Shuffle AUC=0.4966\n",
    "\n",
    "**Same Alpaca filtering pipeline as LFM2.5 & 0.5B notebooks** for fair comparison.\n",
    "\n",
    "‚ö° T4 GPU, ~60-90 min total"
])

# ‚îÄ‚îÄ 1. Setup ‚îÄ‚îÄ
md(["## 1 ¬∑ Setup"])
code([
    "import subprocess, sys\n",
    "for p in ['transformers>=4.40.0','accelerate>=0.25.0','scikit-learn>=1.3.0',\n",
    "           'datasets','tqdm','matplotlib','seaborn']:\n",
    "    subprocess.check_call([sys.executable,'-m','pip','install','-q',p])\n",
    "\n",
    "import os, json, re, pickle, warnings, gc, ast\n",
    "from pathlib import Path\n",
    "import numpy as np, torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "\n",
    "MODEL_ID = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "DOMAINS  = ['math','code','search','translation']\n",
    "SEED = 42; np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "OUT = Path('outputs'); OUT.mkdir(exist_ok=True)\n",
    "CKPT = OUT/'ckpt_qwen15b'; CKPT.mkdir(exist_ok=True)\n",
    "\n",
    "TOOL_S = '<functioncall>'\n",
    "TOOL_E = '</functioncall>'\n",
    "\n",
    "def save_ckpt(tag, obj):\n",
    "    p=CKPT/f'{tag}.pkl'\n",
    "    with open(p,'wb') as f: pickle.dump(obj,f)\n",
    "    print(f'  üíæ {tag} ({p.stat().st_size//1024}KB)')\n",
    "def load_ckpt(tag):\n",
    "    p=CKPT/f'{tag}.pkl'\n",
    "    if p.exists():\n",
    "        with open(p,'rb') as f: o=pickle.load(f)\n",
    "        print(f'  ‚ôªÔ∏è  {tag} (cached)'); return o\n",
    "    return None\n",
    "print('‚úÖ Setup done.')"
])

# ‚îÄ‚îÄ 2. Data Pipeline (SAME AS LFM2.5) ‚îÄ‚îÄ
md([
    "## 2 ¬∑ Data Pipeline (Alpaca Filtering)\n",
    "**Identical to LFM2.5 notebook**: Download Alpaca ‚Üí keyword filtering ‚Üí balanced splits.\n",
    "\n",
    "CAL(320) + TRAIN(320) + VALID(320) + TEST(640) = 1,600 samples"
])
code([
    "from datasets import load_dataset\n",
    "\n",
    "print('Downloading Alpaca dataset...')\n",
    "alpaca = load_dataset('tatsu-lab/alpaca', split='train')\n",
    "print(f'  Alpaca: {len(alpaca)} samples')\n",
    "\n",
    "# ‚îÄ‚îÄ Domain classification + tool-necessity labeling (SAME AS LFM2.5) ‚îÄ‚îÄ\n",
    "_MATH_KW = re.compile(r'(calcul|comput|solve|equation|formula|\\bsum\\b|product|divid|multiply|percent|fraction|area|volume|circumferen|triangle|circle|rectangle|convert.*to|\\bhow much\\b|\\bhow many\\b|average|median|ratio|proportion|interest|mortgage|speed|distance|temperature|celsius|fahrenheit|kilometer|gallon|\\bcos\\b|\\bsin\\b|\\btan\\b|factorial|logarithm|square root|hypotenuse|diagonal|perimeter|probability)', re.I)\n",
    "_CODE_KW = re.compile(r'(python|javascript|java\\b|\\bc\\+\\+|\\bhtml\\b|\\bcss\\b|\\bsql\\b|function|script|algorithm|debug|compil|execut|output|variable|\\bloop\\b|\\barray\\b|\\bclass\\b|\\bimport\\b|\\bprint\\b|\\bsort\\b|\\bcode\\b|program|def\\s|for\\s.*in\\s|while\\s|if\\s.*else)', re.I)\n",
    "_SRCH_KW = re.compile(r'(who (is|was|are|were)|what (year|country|city)|when (did|was)|where (is|was)|capital of|population of|president of|founded|invented|discovered|located|\\bfind\\b.*about|search for|look up|latest|current|recent|today)', re.I)\n",
    "_TRNS_KW = re.compile(r'(translat|\\bin spanish\\b|\\bin french\\b|\\bin german\\b|\\bin chinese\\b|\\bin japanese\\b|\\bin korean\\b|\\bin arabic\\b|\\bin italian\\b|\\bin portuguese\\b|\\bin russian\\b|how do you say|\\b√†\\b|\\b√ºber\\b|en fran[c√ß]ais)', re.I)\n",
    "_CONCEPT = re.compile(r'(explain|describe|what is the (concept|difference|significance|meaning|history|importance)|why (is|are|do|does)|how does.*work|compare and contrast|pros and cons|advantages|definition of)', re.I)\n",
    "_HAS_NUM = re.compile(r'\\d+\\.?\\d*')\n",
    "\n",
    "def classify(inst, inp=''):\n",
    "    t = (inst+' '+inp).strip()\n",
    "    tl = t.lower()\n",
    "    scores = {'math': len(_MATH_KW.findall(tl)),\n",
    "              'code': len(_CODE_KW.findall(tl)),\n",
    "              'search': len(_SRCH_KW.findall(tl)),\n",
    "              'translation': len(_TRNS_KW.findall(tl))}\n",
    "    dom = max(scores, key=scores.get)\n",
    "    if scores[dom] == 0: return None, None\n",
    "    is_concept = bool(_CONCEPT.search(tl))\n",
    "    if dom == 'math':\n",
    "        has_num = bool(_HAS_NUM.search(t))\n",
    "        has_verb = bool(re.search(r'(calcul|comput|solve|convert|find the|determine|how much|how many)', tl))\n",
    "        label = 1 if (has_num and has_verb and not is_concept) else 0\n",
    "    elif dom == 'code':\n",
    "        has_action = bool(re.search(r'(write|create|implement|build|generate|debug|fix|run|execute|test|develop)', tl))\n",
    "        label = 1 if (has_action and not is_concept) else 0\n",
    "    elif dom == 'search':\n",
    "        label = 0 if is_concept else 1\n",
    "    elif dom == 'translation':\n",
    "        has_target = bool(re.search(r'(translat.*to|in (spanish|french|german|chinese|japanese|korean|arabic|italian|portuguese|russian))', tl))\n",
    "        label = 1 if (has_target and not is_concept) else 0\n",
    "    else:\n",
    "        return None, None\n",
    "    return dom, label\n",
    "\n",
    "filtered = {d: {0: [], 1: []} for d in DOMAINS}\n",
    "for row in alpaca:\n",
    "    inst = row['instruction']\n",
    "    inp  = row.get('input', '')\n",
    "    dom, label = classify(inst, inp)\n",
    "    if dom is None: continue\n",
    "    text = inst + ('\\n' + inp if inp else '')\n",
    "    filtered[dom][label].append(text)\n",
    "\n",
    "print('\\nFiltered counts:')\n",
    "for d in DOMAINS:\n",
    "    print(f'  {d:12s}: tool={len(filtered[d][1]):4d}  non-tool={len(filtered[d][0]):4d}')\n",
    "\n",
    "PER_DOM = {'cal': (40,40), 'train': (40,40), 'valid': (40,40), 'test': (80,80)}\n",
    "all_data = {}\n",
    "sample_id = 0\n",
    "for split, (nt, nn) in PER_DOM.items():\n",
    "    all_data[split] = []\n",
    "\n",
    "for d in DOMAINS:\n",
    "    for label in [0, 1]:\n",
    "        random_pool = filtered[d][label].copy()\n",
    "        np.random.shuffle(random_pool)\n",
    "        idx = 0\n",
    "        for split, (nt, nn) in PER_DOM.items():\n",
    "            need = nt if label == 1 else nn\n",
    "            for _ in range(need):\n",
    "                if idx < len(random_pool):\n",
    "                    text = random_pool[idx]; idx += 1\n",
    "                else:\n",
    "                    text = f\"[placeholder {d} {'tool' if label else 'nontool'} {sample_id}]\"\n",
    "                all_data[split].append({\n",
    "                    'id': f'{d}_{split}_{label}_{sample_id}',\n",
    "                    'instruction': text, 'domain': d, 'label': label})\n",
    "                sample_id += 1\n",
    "\n",
    "for split in all_data:\n",
    "    np.random.shuffle(all_data[split])\n",
    "\n",
    "print('\\nSplit sizes:')\n",
    "for split, samples in all_data.items():\n",
    "    t = sum(1 for s in samples if s['label']==1)\n",
    "    print(f'  {split:5s}: {len(samples)} ({t} tool / {len(samples)-t} non-tool)')\n",
    "\n",
    "all_ids = set()\n",
    "for split in all_data:\n",
    "    for s in all_data[split]:\n",
    "        assert s['id'] not in all_ids; all_ids.add(s['id'])\n",
    "print(f'Total: {len(all_ids)} unique samples ‚úÖ')"
])

# ‚îÄ‚îÄ 3. Tools & System Prompt ‚îÄ‚îÄ
md(["## 3 ¬∑ Tools & System Prompt"])
code([
    "TOOLS = [\n",
    "  {'name':'calculator','description':'Evaluate a mathematical expression and return the numeric result.',\n",
    "   'parameters':{'type':'object','properties':{'expression':{'type':'string','description':'Math expression'}},'required':['expression']}},\n",
    "  {'name':'python_interpreter','description':'Execute Python code and return the output.',\n",
    "   'parameters':{'type':'object','properties':{'code':{'type':'string','description':'Python source code'}},'required':['code']}},\n",
    "  {'name':'web_search','description':'Search the web for up-to-date information.',\n",
    "   'parameters':{'type':'object','properties':{'query':{'type':'string','description':'Search query'}},'required':['query']}},\n",
    "  {'name':'translator','description':'Translate text from one language to another.',\n",
    "   'parameters':{'type':'object','properties':{'text':{'type':'string','description':'Text to translate'},\n",
    "    'target_language':{'type':'string','description':'Target language'}},'required':['text','target_language']}}\n",
    "]\n",
    "TOOL_NAMES = {t['name'] for t in TOOLS}\n",
    "tool_json = json.dumps(TOOLS, indent=2)\n",
    "SYS_PROMPT = (\n",
    "    'You are a helpful assistant with access to tools. '\n",
    "    'When a user request requires using a tool, generate a tool call '\n",
    "    f'using {TOOL_S} token. Available tools:\\n' + tool_json)\n",
    "\n",
    "def fmt(sample):\n",
    "    return [{'role':'system','content':SYS_PROMPT},\n",
    "            {'role':'user','content':sample['instruction']}]\n",
    "print(f'System prompt: {len(SYS_PROMPT)} chars, {len(TOOLS)} tools')"
])

# ‚îÄ‚îÄ 4. Load Model ‚îÄ‚îÄ
md(["## 4 ¬∑ Load Model\n","Qwen2.5-1.5B-Instruct: 28 layers, ~1.5B params, pure Transformer.\n","This is the paper's **main model** ‚Äî direct comparison possible."])
code([
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map='auto')\n",
    "model.eval()\n",
    "NUM_LAYERS = len(model.model.layers)\n",
    "print(f'‚úÖ {MODEL_ID} ({sum(p.numel() for p in model.parameters())/1e6:.0f}M params, {NUM_LAYERS} layers)')\n",
    "assert NUM_LAYERS == 28, f'Expected 28 layers, got {NUM_LAYERS}'"
])

# ‚îÄ‚îÄ 5. Hidden State Extraction ‚îÄ‚îÄ
md(["## 5 ¬∑ Hidden State Extraction\n","Last-token hidden states at all 28 layers. Checkpointed."])
code([
    "LAYERS = list(range(NUM_LAYERS))\n",
    "\n",
    "def extract_h(samples, tag):\n",
    "    cached = load_ckpt(f'h_{tag}')\n",
    "    if cached is not None: return cached\n",
    "    states = {l: [] for l in LAYERS}\n",
    "    hooks = []\n",
    "    def make_hook(li):\n",
    "        def fn(m, inp, out):\n",
    "            h = out[0] if isinstance(out, tuple) else out\n",
    "            states[li].append(h[:,-1,:].detach().cpu().float().numpy())\n",
    "        return fn\n",
    "    for l in LAYERS:\n",
    "        hooks.append(model.model.layers[l].register_forward_hook(make_hook(l)))\n",
    "    try:\n",
    "        for s in tqdm(samples, desc=tag):\n",
    "            text = tokenizer.apply_chat_template(fmt(s), tokenize=False, add_generation_prompt=True)\n",
    "            inp = tokenizer(text, return_tensors='pt', truncation=True, max_length=2048).to(DEVICE)\n",
    "            with torch.no_grad(): model(**inp)\n",
    "            if DEVICE.type=='cuda': torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        for h in hooks: h.remove()\n",
    "    result = {l: np.concatenate(states[l], axis=0) for l in LAYERS}\n",
    "    save_ckpt(f'h_{tag}', result)\n",
    "    return result\n",
    "\n",
    "cal_h   = extract_h(all_data['cal'],   'cal')\n",
    "train_h = extract_h(all_data['train'], 'train')\n",
    "valid_h = extract_h(all_data['valid'], 'valid')\n",
    "test_h  = extract_h(all_data['test'],  'test')\n",
    "\n",
    "cal_y   = np.array([s['label']  for s in all_data['cal']])\n",
    "cal_d   = np.array([s['domain'] for s in all_data['cal']])\n",
    "train_y = np.array([s['label']  for s in all_data['train']])\n",
    "train_d = np.array([s['domain'] for s in all_data['train']])\n",
    "valid_y = np.array([s['label']  for s in all_data['valid']])\n",
    "valid_d = np.array([s['domain'] for s in all_data['valid']])\n",
    "test_y  = np.array([s['label']  for s in all_data['test']])\n",
    "test_d  = np.array([s['domain'] for s in all_data['test']])\n",
    "print('‚úÖ All hidden states ready.')"
])

# ‚îÄ‚îÄ 6. Probe Sweep ‚îÄ‚îÄ
md(["## 6 ¬∑ Probe Sweep ‚Üí L*\n","Paper target: L*=18, AUC=0.9996, Shuffle AUC=0.4966 [0.4884, 0.5053]"])
code([
    "aucs = {}\n",
    "for l in LAYERS:\n",
    "    sc = StandardScaler()\n",
    "    Xtr = sc.fit_transform(train_h[l]); Xva = sc.transform(valid_h[l])\n",
    "    p = LogisticRegression(max_iter=1000, C=1.0, solver='lbfgs')\n",
    "    p.fit(Xtr, train_y)\n",
    "    auc = roc_auc_score(valid_y, p.predict_proba(Xva)[:,1])\n",
    "    acc = accuracy_score(valid_y, p.predict(Xva))\n",
    "    aucs[l] = auc\n",
    "    print(f'  Layer {l:2d} | AUC: {auc:.4f} | Acc: {acc:.4f}')\n",
    "\n",
    "best_auc = max(aucs.values())\n",
    "candidates = [l for l in LAYERS if aucs[l] >= best_auc - 0.005]\n",
    "L_STAR = max(candidates)\n",
    "print(f'\\nüèÜ L* = {L_STAR} (AUC={aucs[L_STAR]:.4f})')\n",
    "print(f'   Paper target: L*=18, AUC=0.9996')\n",
    "\n",
    "# Shuffle AUC\n",
    "N_SHUFFLE = 100\n",
    "sc_s = StandardScaler()\n",
    "Xtr_s = sc_s.fit_transform(train_h[L_STAR])\n",
    "Xva_s = sc_s.transform(valid_h[L_STAR])\n",
    "shuffle_list = []\n",
    "for _ in range(N_SHUFFLE):\n",
    "    py = np.random.permutation(train_y)\n",
    "    ps = LogisticRegression(max_iter=1000, C=1.0, solver='lbfgs'); ps.fit(Xtr_s, py)\n",
    "    shuffle_list.append(roc_auc_score(valid_y, ps.predict_proba(Xva_s)[:,1]))\n",
    "shuf_mean = np.mean(shuffle_list)\n",
    "shuf_ci = (np.percentile(shuffle_list, 2.5), np.percentile(shuffle_list, 97.5))\n",
    "print(f'   Shuffle AUC: {shuf_mean:.4f} [{shuf_ci[0]:.4f}, {shuf_ci[1]:.4f}]')\n",
    "print(f'   Paper: 0.4966 [0.4884, 0.5053]')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.bar(LAYERS, [aucs[l] for l in LAYERS], color='#2196F3', alpha=0.85)\n",
    "ax.axvline(L_STAR, color='gold', lw=2, ls='--', label=f'L*={L_STAR}')\n",
    "ax.axhline(0.5, color='gray', lw=1, ls=':')\n",
    "ax.set_xlabel('Layer'); ax.set_ylabel('AUC')\n",
    "ax.set_title(f'Probe Sweep: Qwen2.5-1.5B (Paper: L*=18, AUC=0.9996)')\n",
    "ax.legend(); ax.set_xticks(LAYERS); plt.tight_layout()\n",
    "plt.savefig(OUT/'probe_sweep_qwen15b.png', dpi=150); plt.show()"
])

# ‚îÄ‚îÄ 7. Steering Vectors ‚îÄ‚îÄ
md(["## 7 ¬∑ Steering Vectors (CAL split)"])
code([
    "H = cal_h[L_STAR]\n",
    "tool_m = cal_y == 1\n",
    "v_global = H[tool_m].mean(0) - H[~tool_m].mean(0)\n",
    "v_global = v_global / (np.linalg.norm(v_global) + 1e-8)\n",
    "\n",
    "d_vecs = {}\n",
    "for d in DOMAINS:\n",
    "    dm = cal_d == d\n",
    "    vd = H[dm & tool_m].mean(0) - H[dm & ~tool_m].mean(0)\n",
    "    vd = vd / (np.linalg.norm(vd) + 1e-8)\n",
    "    d_vecs[d] = vd\n",
    "    print(f'  {d:12s} cos(v_d, v_g) = {np.dot(vd, v_global):.4f}')\n",
    "\n",
    "print('\\nCross-domain cosines:')\n",
    "for i,d1 in enumerate(DOMAINS):\n",
    "    for d2 in DOMAINS[i+1:]:\n",
    "        print(f'  {d1}‚Üî{d2}: {np.dot(d_vecs[d1], d_vecs[d2]):.4f}')\n",
    "print('‚úÖ Vectors built.')"
])

# ‚îÄ‚îÄ 8. Router & Probes ‚îÄ‚îÄ
md(["## 8 ¬∑ Router & Probes (TRAIN split)"])
code([
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(train_h[L_STAR])\n",
    "d2i = {d:i for i,d in enumerate(DOMAINS)}\n",
    "i2d = {i:d for d,i in d2i.items()}\n",
    "\n",
    "router = LogisticRegression(max_iter=2000, C=1.0, solver='lbfgs', multi_class='multinomial')\n",
    "router.fit(X_tr, np.array([d2i[d] for d in train_d]))\n",
    "print(f'  Router train acc: {accuracy_score([d2i[d] for d in train_d], router.predict(X_tr)):.4f}')\n",
    "\n",
    "probes = {}\n",
    "for d in DOMAINS:\n",
    "    m = train_d == d; Xd = X_tr[m]; yd = train_y[m]\n",
    "    if len(np.unique(yd)) < 2:\n",
    "        print(f'  ‚ö†Ô∏è Probe {d}: skip'); continue\n",
    "    p = LogisticRegression(max_iter=1000, C=1.0, solver='lbfgs'); p.fit(Xd, yd)\n",
    "    probes[d] = p\n",
    "    print(f'  Probe {d} train acc: {accuracy_score(yd, p.predict(Xd)):.4f}')\n",
    "\n",
    "X_va = scaler.transform(valid_h[L_STAR])\n",
    "print(f'  Router valid acc: {accuracy_score([d2i[d] for d in valid_d], router.predict(X_va)):.4f}')\n",
    "for d in DOMAINS:\n",
    "    if d not in probes: continue\n",
    "    m = valid_d == d\n",
    "    if m.sum()==0: continue\n",
    "    print(f'  Probe {d} valid acc: {accuracy_score(valid_y[m], probes[d].predict(X_va[m])):.4f}')\n",
    "print('‚úÖ Router & probes trained.')"
])

# ‚îÄ‚îÄ 9. HP Tuning ‚îÄ‚îÄ
md(["## 9 ¬∑ Hyperparameter Tuning (VALID)\n","Œ± sweep ‚Üí œÑ grid ‚Üí Œ≤ sweep. Paper uses **Œ±=4.0** for Qwen2.5-1.5B."])
code([
    "def eval_hidden(alpha, tau, beta, h, y, d_arr):\n",
    "    X = scaler.transform(h); preds = []\n",
    "    for i in range(len(y)):\n",
    "        xi = X[i:i+1]\n",
    "        dom = i2d[router.predict(xi)[0]]\n",
    "        if dom not in probes: preds.append(0); continue\n",
    "        pt = probes[dom].predict_proba(xi)[0,1]\n",
    "        gate = 1 if pt >= tau else (-1 if pt <= 1-tau else 0)\n",
    "        preds.append(1 if gate == 1 else 0)\n",
    "    preds = np.array(preds)\n",
    "    return {'f1': f1_score(y, preds, zero_division=0),\n",
    "            'precision': precision_score(y, preds, zero_division=0),\n",
    "            'recall': recall_score(y, preds, zero_division=0),\n",
    "            'fpr': (preds[y==0]==1).mean() if (y==0).sum()>0 else 0,\n",
    "            'accuracy': accuracy_score(y, preds)}\n",
    "\n",
    "print('Œ± sweep:')\n",
    "alpha_res = []\n",
    "for a in [1,2,3,4,5,6,7,8,10,12,15,20]:\n",
    "    m = eval_hidden(a, 0.60, 0.3, valid_h[L_STAR], valid_y, valid_d)\n",
    "    m['a'] = a; alpha_res.append(m)\n",
    "    print(f'  Œ±={a:4.0f} | F1={m[\"f1\"]:.4f} Prec={m[\"precision\"]:.4f} Rec={m[\"recall\"]:.4f} FPR={m[\"fpr\"]:.4f}')\n",
    "ALPHA = max(alpha_res, key=lambda x: x['f1'])['a']\n",
    "print(f'  Best Œ± = {ALPHA}')\n",
    "\n",
    "print('\\nœÑ sweep:')\n",
    "tau_res = []\n",
    "for t in [0.50, 0.55, 0.60, 0.65, 0.70]:\n",
    "    m = eval_hidden(ALPHA, t, 0.3, valid_h[L_STAR], valid_y, valid_d)\n",
    "    m['t'] = t; tau_res.append(m)\n",
    "    print(f'  œÑ={t:.2f} | F1={m[\"f1\"]:.4f} Prec={m[\"precision\"]:.4f} Rec={m[\"recall\"]:.4f} FPR={m[\"fpr\"]:.4f}')\n",
    "TAU = max(tau_res, key=lambda x: x['f1'])['t']\n",
    "print(f'  Best œÑ = {TAU}')\n",
    "\n",
    "print('\\nŒ≤ sweep:')\n",
    "beta_res = []\n",
    "for b in [0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]:\n",
    "    m = eval_hidden(ALPHA, TAU, b, valid_h[L_STAR], valid_y, valid_d)\n",
    "    m['b'] = b; beta_res.append(m)\n",
    "    print(f'  Œ≤={b:.1f} | F1={m[\"f1\"]:.4f} Prec={m[\"precision\"]:.4f} Rec={m[\"recall\"]:.4f} FPR={m[\"fpr\"]:.4f}')\n",
    "BETA = max(beta_res, key=lambda x: x['f1'])['b']\n",
    "print(f'  Best Œ≤ = {BETA}\\n  Final: Œ±={ALPHA}, œÑ={TAU}, Œ≤={BETA}')\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(15,4))\n",
    "axes[0].plot([r['a'] for r in alpha_res],[r['f1'] for r in alpha_res],'o-')\n",
    "axes[0].axvline(ALPHA,color='gold',ls='--'); axes[0].set_xlabel('Œ±'); axes[0].set_ylabel('F1')\n",
    "axes[1].plot([r['t'] for r in tau_res],[r['f1'] for r in tau_res],'o-',color='#FF5722')\n",
    "axes[1].axvline(TAU,color='gold',ls='--'); axes[1].set_xlabel('œÑ'); axes[1].set_ylabel('F1')\n",
    "axes[2].plot([r['b'] for r in beta_res],[r['f1'] for r in beta_res],'o-',color='#4CAF50')\n",
    "axes[2].axvline(BETA,color='gold',ls='--'); axes[2].set_xlabel('Œ≤'); axes[2].set_ylabel('F1')\n",
    "plt.tight_layout(); plt.savefig(OUT/'hp_sweep_qwen05b.png',dpi=150); plt.show()"
])

# ‚îÄ‚îÄ 10. TEST Evaluation ‚îÄ‚îÄ
md(["## 10 ¬∑ TEST Evaluation\n","640 samples, greedy decoding, trigger=`<functioncall>` detection."])
code([
    "def parse_tool_call(text):\n",
    "    if TOOL_S not in text: return None\n",
    "    try:\n",
    "        s = text.index(TOOL_S) + len(TOOL_S)\n",
    "        e = text.index(TOOL_E) if TOOL_E in text else len(text)\n",
    "        raw = text[s:e].strip()\n",
    "        try: obj = json.loads(raw); json_ok = True\n",
    "        except: obj = None; json_ok = False\n",
    "        if not json_ok:\n",
    "            try: obj = ast.literal_eval(raw); json_ok = True\n",
    "            except: pass\n",
    "        if obj and isinstance(obj, dict):\n",
    "            name = obj.get('name','')\n",
    "            args = obj.get('arguments', obj.get('args', {}))\n",
    "            return {'json_valid':True, 'schema_ok': name in TOOL_NAMES,\n",
    "                    'args_ok': isinstance(args,dict) and len(args)>0 and all(v!='' for v in args.values()),\n",
    "                    'name': name, 'args': args}\n",
    "        return {'json_valid': json_ok, 'schema_ok': False, 'args_ok': False, 'name':'', 'args':{}}\n",
    "    except: return {'json_valid':False, 'schema_ok':False, 'args_ok':False, 'name':'', 'args':{}}\n",
    "\n",
    "def generate(messages, hook_fn=None, layer=None, max_tok=256):\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inp = tokenizer(text, return_tensors='pt', truncation=True, max_length=2048)\n",
    "    inp = {k:v.to(DEVICE) for k,v in inp.items()}\n",
    "    hook = None\n",
    "    if hook_fn and layer is not None:\n",
    "        hook = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inp, max_new_tokens=max_tok, do_sample=False,\n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "        return tokenizer.decode(out[0][inp['input_ids'].shape[1]:], skip_special_tokens=False)\n",
    "    finally:\n",
    "        if hook: hook.remove()\n",
    "\n",
    "_injected = False; _info = {}\n",
    "def asa_hook(module, inp, out):\n",
    "    global _injected, _info\n",
    "    if _injected: return out\n",
    "    h = out[0] if isinstance(out, tuple) else out\n",
    "    rest = out[1:] if isinstance(out, tuple) else None\n",
    "    hl = h[:,-1,:].detach().cpu().float().numpy()\n",
    "    hs = scaler.transform(hl)\n",
    "    dom = i2d[router.predict(hs)[0]]\n",
    "    pt = probes[dom].predict_proba(hs)[0,1] if dom in probes else 0.5\n",
    "    gate = 1 if pt >= TAU else (-1 if pt <= 1-TAU else 0)\n",
    "    _info = {'domain':dom, 'p_tool':float(pt), 'gate':gate}\n",
    "    _injected = True\n",
    "    if gate == 0: return out\n",
    "    vd = d_vecs[dom]; v = (1-BETA)*vd + BETA*v_global\n",
    "    v = v / (np.linalg.norm(v)+1e-8)\n",
    "    vt = torch.tensor(v, dtype=torch.float16).to(h.device)\n",
    "    hn = h.clone(); hn[:,-1,:] = h[:,-1,:] + gate * ALPHA * vt\n",
    "    return (hn,)+rest if rest else hn\n",
    "\n",
    "print(f'Evaluating TEST ({len(all_data[\"test\"])} samples)...')\n",
    "print(f'Config: Œ±={ALPHA}, œÑ={TAU}, Œ≤={BETA}, L*={L_STAR}')\n",
    "bl_res, asa_res = [], []\n",
    "test_samples = all_data['test']\n",
    "\n",
    "for s in tqdm(test_samples, desc='TEST'):\n",
    "    msgs = fmt(s)\n",
    "    bl_out = generate(msgs)\n",
    "    bl_trig = TOOL_S in bl_out\n",
    "    bl_parsed = parse_tool_call(bl_out) if bl_trig else None\n",
    "    bl_res.append({'label':s['label'],'domain':s['domain'],'triggered':bl_trig,'parsed':bl_parsed})\n",
    "    _injected = False; _info = {}\n",
    "    asa_out = generate(msgs, hook_fn=asa_hook, layer=L_STAR)\n",
    "    asa_trig = TOOL_S in asa_out\n",
    "    asa_parsed = parse_tool_call(asa_out) if asa_trig else None\n",
    "    asa_res.append({'label':s['label'],'domain':s['domain'],'triggered':asa_trig,'parsed':asa_parsed,'info':_info})\n",
    "    if DEVICE.type=='cuda': torch.cuda.empty_cache()\n",
    "\n",
    "def compute_metrics(results, name):\n",
    "    y = np.array([r['label'] for r in results])\n",
    "    p = np.array([1 if r['triggered'] else 0 for r in results])\n",
    "    m = {'precision': precision_score(y,p,zero_division=0),\n",
    "         'recall': recall_score(y,p,zero_division=0),\n",
    "         'f1': f1_score(y,p,zero_division=0),\n",
    "         'fpr': (p[y==0]==1).mean() if (y==0).sum()>0 else 0,\n",
    "         'accuracy': accuracy_score(y,p)}\n",
    "    triggered = [r for r in results if r['triggered'] and r['parsed']]\n",
    "    if triggered:\n",
    "        m['succ_json'] = np.mean([r['parsed']['json_valid'] for r in triggered])\n",
    "        m['succ_schema'] = np.mean([r['parsed']['schema_ok'] for r in triggered])\n",
    "        m['succ_args'] = np.mean([r['parsed']['args_ok'] for r in triggered])\n",
    "    else:\n",
    "        m['succ_json'] = m['succ_schema'] = m['succ_args'] = 0.0\n",
    "    print(f'\\n[{name}]')\n",
    "    print(f'  Trig P/R/F1: {m[\"precision\"]:.4f} / {m[\"recall\"]:.4f} / {m[\"f1\"]:.4f}')\n",
    "    print(f'  FPR: {m[\"fpr\"]:.4f}  Acc: {m[\"accuracy\"]:.4f}')\n",
    "    print(f'  Success P: JSON={m[\"succ_json\"]:.4f} Schema={m[\"succ_schema\"]:.4f} Args={m[\"succ_args\"]:.4f}')\n",
    "    return m\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "bl_m = compute_metrics(bl_res, 'Baseline')\n",
    "asa_m = compute_metrics(asa_res, 'ASA')\n",
    "\n",
    "print('\\nPer-Domain ASA:')\n",
    "for d in DOMAINS:\n",
    "    dr = [r for r in asa_res if r['domain']==d]\n",
    "    if dr: compute_metrics(dr, f'ASA/{d}')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "names = ['Precision','Recall','F1']\n",
    "bv = [bl_m['precision'],bl_m['recall'],bl_m['f1']]\n",
    "av = [asa_m['precision'],asa_m['recall'],asa_m['f1']]\n",
    "x = np.arange(len(names))\n",
    "ax.bar(x-0.2,bv,0.35,label='Baseline',color='#90A4AE')\n",
    "ax.bar(x+0.2,av,0.35,label='ASA',color='#FF5722')\n",
    "ax.set_xticks(x); ax.set_xticklabels(names); ax.set_ylabel('Score')\n",
    "ax.set_title('Baseline vs ASA: Qwen2.5-1.5B'); ax.legend(); ax.set_ylim(0,1.05)\n",
    "plt.tight_layout(); plt.savefig(OUT/'baseline_vs_asa_qwen05b.png',dpi=150); plt.show()"
])

# ‚îÄ‚îÄ 11. Ablation ‚îÄ‚îÄ
md(["## 11 ¬∑ Ablation Study"])
code([
    "print('Ablation on TEST hidden states...')\n",
    "test_H = test_h[L_STAR]\n",
    "\n",
    "def ablation_eval(variant):\n",
    "    X = scaler.transform(test_H); preds = []\n",
    "    for i in range(len(test_y)):\n",
    "        xi = X[i:i+1]\n",
    "        dom = i2d[router.predict(xi)[0]]\n",
    "        if dom not in probes: preds.append(0); continue\n",
    "        pt = probes[dom].predict_proba(xi)[0,1]\n",
    "        if variant == 'full':\n",
    "            gate = 1 if pt >= TAU else (-1 if pt <= 1-TAU else 0)\n",
    "        elif variant == 'no_gate':\n",
    "            gate = 1\n",
    "        else:\n",
    "            gate = 1 if pt >= TAU else (-1 if pt <= 1-TAU else 0)\n",
    "        preds.append(1 if gate == 1 else 0)\n",
    "    preds = np.array(preds)\n",
    "    return {'f1': f1_score(test_y,preds,zero_division=0),\n",
    "            'precision': precision_score(test_y,preds,zero_division=0),\n",
    "            'recall': recall_score(test_y,preds,zero_division=0),\n",
    "            'fpr': (preds[test_y==0]==1).mean() if (test_y==0).sum()>0 else 0}\n",
    "\n",
    "for v in ['full','no_gate','global_only','domain_only']:\n",
    "    m = ablation_eval(v)\n",
    "    print(f'{v:15s} | F1={m[\"f1\"]:.4f} | Prec={m[\"precision\"]:.4f} | Rec={m[\"recall\"]:.4f} | FPR={m[\"fpr\"]:.4f}')\n",
    "print('‚úÖ Ablation complete.')"
])

# ‚îÄ‚îÄ 12. Save + Report ‚îÄ‚îÄ
md(["## 12 ¬∑ Save Assets & Final Report"])
code([
    "assets = OUT/'asa_assets_qwen05b'; assets.mkdir(exist_ok=True)\n",
    "vecs = {'global': v_global}; vecs.update(d_vecs)\n",
    "np.savez(assets/'steering_vectors.npz', **vecs)\n",
    "with open(assets/'router.pkl','wb') as f: pickle.dump(router,f)\n",
    "with open(assets/'probes.pkl','wb') as f: pickle.dump(probes,f)\n",
    "with open(assets/'scaler.pkl','wb') as f: pickle.dump(scaler,f)\n",
    "config = {'model': MODEL_ID, 'L_star': int(L_STAR), 'alpha': float(ALPHA),\n",
    "          'tau': float(TAU), 'beta': float(BETA), 'domains': DOMAINS,\n",
    "          'probe_aucs': {str(k):float(v) for k,v in aucs.items()},\n",
    "          'test_baseline': {k:float(v) for k,v in bl_m.items()},\n",
    "          'test_asa': {k:float(v) for k,v in asa_m.items()}}\n",
    "with open(assets/'config.json','w') as f: json.dump(config,f,indent=2)\n",
    "print(f'Assets saved: {assets}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('  FINAL REPORT: Qwen2.5-1.5B-Instruct')\n",
    "print('='*70)\n",
    "print(f'L*={L_STAR}, AUC={aucs[L_STAR]:.4f} (paper: 18, 0.9996)')\n",
    "print(f'Shuffle AUC={shuf_mean:.4f} (paper: 0.4966)')\n",
    "print(f'\\nBaseline: P={bl_m[\"precision\"]:.4f} R={bl_m[\"recall\"]:.4f} F1={bl_m[\"f1\"]:.4f} FPR={bl_m[\"fpr\"]:.4f}')\n",
    "print(f'ASA:      P={asa_m[\"precision\"]:.4f} R={asa_m[\"recall\"]:.4f} F1={asa_m[\"f1\"]:.4f} FPR={asa_m[\"fpr\"]:.4f}')\n",
    "print(f'Config: Œ±={ALPHA}, œÑ={TAU}, Œ≤={BETA}')\n",
    "print('='*70)"
])

# ‚îÄ‚îÄ 13. Demo ‚îÄ‚îÄ
md(["## 13 ¬∑ Demo"])
code([
    "demos = test_samples[:8]\n",
    "for s in demos:\n",
    "    msgs = fmt(s)\n",
    "    bl = generate(msgs)\n",
    "    _injected = False; _info = {}\n",
    "    asa_out = generate(msgs, hook_fn=asa_hook, layer=L_STAR)\n",
    "    label = 'TOOL' if s['label']==1 else 'NO-TOOL'\n",
    "    gs = {1:'+1',-1:'-1',0:'0'}.get(_info.get('gate',0),'?')\n",
    "    print(f'\\n[{label}] {s[\"instruction\"][:80]}')\n",
    "    print(f'  Baseline: {\"TRIGGERED\" if TOOL_S in bl else \"no trigger\"}')\n",
    "    print(f'  ASA:      {\"TRIGGERED\" if TOOL_S in asa_out else \"no trigger\"} '\n",
    "          f'(p={_info.get(\"p_tool\",0):.3f}, gate={gs})')\n",
    "    if DEVICE.type=='cuda': torch.cuda.empty_cache()\n",
    "print(f'\\nDONE!')"
])

# ‚îÄ‚îÄ Write notebook ‚îÄ‚îÄ
nb = {
    "nbformat": 4, "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {"display_name":"Python 3","language":"python","name":"python3"},
        "language_info": {"name":"python","version":"3.10.0"},
        "accelerator": "GPU",
        "colab": {"provenance":[],"gpuType":"T4"}
    },
    "cells": cells
}

out_path = "ASA_Qwen15B_Reproduction.ipynb"
with open(out_path, "w", encoding="utf-8") as f:
    json.dump(nb, f, indent=1, ensure_ascii=False)
print(f"‚úÖ Written {out_path} ({len(cells)} cells)")
