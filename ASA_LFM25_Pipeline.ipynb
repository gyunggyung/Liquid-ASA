{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¬ ASA Ã— LFM2.5-1.2B-Instruct\n",
    "**Training-Free Tool-Calling Enhancement via Activation Steering**\n",
    "\n",
    "This notebook runs the complete ASA pipeline on a Colab T4 GPU.\n",
    "Each cell can be run independently â€” stop and resume anytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.40.0 accelerate>=0.25.0 scikit-learn>=1.3.0 tqdm matplotlib seaborn\n",
    "!git clone https://github.com/gyunggyung/Liquid-ASA.git 2>/dev/null || echo \"Already cloned\"\n",
    "%cd /content/Liquid-ASA\n",
    "!ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, warnings, gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_global_mem / 1e9:.1f} GB\")\n",
    "\n",
    "# Config\n",
    "MODEL_ID  = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
    "DOMAINS   = [\"math\", \"code\", \"search\", \"translation\"]\n",
    "LAYERS    = list(range(16))\n",
    "GQA_START = 10\n",
    "TOOL_S    = \"<|tool_call_start|>\"\n",
    "TOOL_E    = \"<|tool_call_end|>\"\n",
    "SEED      = 42\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUT_DIR  = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "CKPT_DIR = Path(\"outputs/ckpt\"); CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ALPHA, TAU, BETA = 6.0, 0.60, 0.3\n",
    "np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "def save_ckpt(name, obj):\n",
    "    p = CKPT_DIR / f\"{name}.pkl\"\n",
    "    with open(p, \"wb\") as f: pickle.dump(obj, f)\n",
    "    print(f\"  Saved: {name} ({p.stat().st_size/1024:.0f} KB)\")\n",
    "\n",
    "def load_ckpt(name):\n",
    "    p = CKPT_DIR / f\"{name}.pkl\"\n",
    "    if p.exists():\n",
    "        with open(p, \"rb\") as f: obj = pickle.load(f)\n",
    "        print(f\"  Cached: {name}\")\n",
    "        return obj\n",
    "    return None\n",
    "\n",
    "print(\"Config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "\n",
    "tools      = load_json(DATA_DIR / \"tools.json\")\n",
    "cal_data   = load_json(DATA_DIR / \"cal_data.json\")\n",
    "train_data = load_json(DATA_DIR / \"train_data.json\")\n",
    "valid_data = load_json(DATA_DIR / \"valid_data.json\")\n",
    "test_data  = load_json(DATA_DIR / \"test_data.json\")\n",
    "\n",
    "tool_json = json.dumps(tools, indent=2)\n",
    "SYS_PROMPT = (\n",
    "    \"You are a helpful assistant with access to tools. \"\n",
    "    \"When a user request requires using a tool, generate a tool call \"\n",
    "    f\"between {TOOL_S} and {TOOL_E} tokens. Available tools:\\n\" + tool_json\n",
    ")\n",
    "\n",
    "def fmt(sample):\n",
    "    return [{\"role\":\"system\",\"content\":SYS_PROMPT}, {\"role\":\"user\",\"content\":sample[\"instruction\"]}]\n",
    "\n",
    "# Verify splits\n",
    "all_ids = set()\n",
    "for name, ds in [(\"CAL\",cal_data),(\"TRAIN\",train_data),(\"VALID\",valid_data),(\"TEST\",test_data)]:\n",
    "    ids = {s[\"id\"] for s in ds}\n",
    "    assert not (ids & all_ids), f\"Overlap in {name}!\"\n",
    "    all_ids |= ids\n",
    "    t = sum(1 for s in ds if s[\"label\"]==1)\n",
    "    print(f\"  {name:5s}: {len(ds)} samples ({t} tool / {len(ds)-t} non-tool)\")\n",
    "print(\"Data OK - no overlap between splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Loaded: {MODEL_ID} ({sum(p.numel() for p in model.parameters())/1e6:.0f}M params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Hidden States\n",
    "> This is the slowest step (~15 min on T4). Results are checkpointed â€” if you restart, cached data is loaded instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(samples, tag):\n",
    "    cached = load_ckpt(f\"h_{tag}\")\n",
    "    if cached is not None: return cached\n",
    "    states = {l: [] for l in LAYERS}\n",
    "    hooks = []\n",
    "    def make_hook(li):\n",
    "        def fn(mod, inp, out):\n",
    "            h = out[0] if isinstance(out, tuple) else out\n",
    "            states[li].append(h[:, -1, :].detach().cpu().float().numpy())\n",
    "        return fn\n",
    "    for l in LAYERS:\n",
    "        hooks.append(model.model.layers[l].register_forward_hook(make_hook(l)))\n",
    "    try:\n",
    "        for s in tqdm(samples, desc=tag):\n",
    "            text = tokenizer.apply_chat_template(fmt(s), tokenize=False, add_generation_prompt=True)\n",
    "            inp = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "            with torch.no_grad(): model(**inp)\n",
    "            if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        for h in hooks: h.remove()\n",
    "    result = {l: np.concatenate(states[l], axis=0) for l in LAYERS}\n",
    "    save_ckpt(f\"h_{tag}\", result)\n",
    "    return result\n",
    "\n",
    "cal_h   = extract(cal_data,   \"cal\")\n",
    "train_h = extract(train_data, \"train\")\n",
    "valid_h = extract(valid_data, \"valid\")\n",
    "test_h  = extract(test_data,  \"test\")\n",
    "\n",
    "cal_y,   cal_d   = np.array([s[\"label\"] for s in cal_data]),   np.array([s[\"domain\"] for s in cal_data])\n",
    "train_y, train_d = np.array([s[\"label\"] for s in train_data]), np.array([s[\"domain\"] for s in train_data])\n",
    "valid_y, valid_d = np.array([s[\"label\"] for s in valid_data]), np.array([s[\"domain\"] for s in valid_data])\n",
    "test_y,  test_d  = np.array([s[\"label\"] for s in test_data]),  np.array([s[\"domain\"] for s in test_data])\n",
    "print(\"All hidden states ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probe Sweep â†’ Find Optimal Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = {}\n",
    "for l in LAYERS:\n",
    "    sc = StandardScaler()\n",
    "    Xtr = sc.fit_transform(train_h[l])\n",
    "    Xva = sc.transform(valid_h[l])\n",
    "    p = LogisticRegression(max_iter=1000, C=1.0, solver=\"lbfgs\")\n",
    "    p.fit(Xtr, train_y)\n",
    "    auc = roc_auc_score(valid_y, p.predict_proba(Xva)[:, 1])\n",
    "    acc = accuracy_score(valid_y, p.predict(Xva))\n",
    "    aucs[l] = auc\n",
    "    tag = \"GQA\" if l >= GQA_START else \"LIV\"\n",
    "    print(f\"  Layer {l:2d} ({tag}) | AUC: {auc:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "L_STAR = max(aucs, key=aucs.get)\n",
    "print(f\"\\nBest layer: L{L_STAR} (AUC = {aucs[L_STAR]:.4f})\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "colors = ['#2196F3' if l < GQA_START else '#FF5722' for l in LAYERS]\n",
    "ax.bar(LAYERS, [aucs[l] for l in LAYERS], color=colors, alpha=0.85)\n",
    "ax.axvline(x=L_STAR, color='gold', linewidth=2, linestyle='--', label=f'L*={L_STAR}')\n",
    "ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"AUC\"); ax.set_title(\"Probe Sweep (Blue=LIV, Red=GQA)\")\n",
    "ax.legend(); ax.set_xticks(LAYERS); plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"probe_sweep.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = cal_h[L_STAR]\n",
    "tool_mask = cal_y == 1\n",
    "v_global = H[tool_mask].mean(0) - H[~tool_mask].mean(0)\n",
    "v_global = v_global / (np.linalg.norm(v_global) + 1e-8)\n",
    "\n",
    "domain_vecs = {}\n",
    "for d in DOMAINS:\n",
    "    dm = cal_d == d\n",
    "    vd = H[dm & tool_mask].mean(0) - H[dm & ~tool_mask].mean(0)\n",
    "    vd = vd / (np.linalg.norm(vd) + 1e-8)\n",
    "    domain_vecs[d] = vd\n",
    "    print(f\"  {d:12s} | cos(v_d, v_global) = {np.dot(vd, v_global):.4f}\")\n",
    "print(\"Vectors built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Router & Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(train_h[L_STAR])\n",
    "d2i = {d: i for i, d in enumerate(DOMAINS)}\n",
    "i2d = {i: d for d, i in d2i.items()}\n",
    "\n",
    "router = LogisticRegression(max_iter=2000, C=1.0, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "router.fit(X_tr, np.array([d2i[d] for d in train_d]))\n",
    "print(f\"  Router train acc: {accuracy_score(np.array([d2i[d] for d in train_d]), router.predict(X_tr)):.4f}\")\n",
    "\n",
    "probes = {}\n",
    "for d in DOMAINS:\n",
    "    m = train_d == d\n",
    "    p = LogisticRegression(max_iter=1000, C=1.0, solver=\"lbfgs\")\n",
    "    p.fit(X_tr[m], train_y[m])\n",
    "    probes[d] = p\n",
    "    print(f\"  Probe '{d}' train acc: {accuracy_score(train_y[m], p.predict(X_tr[m])):.4f}\")\n",
    "\n",
    "X_va = scaler.transform(valid_h[L_STAR])\n",
    "print(f\"  Router valid acc: {accuracy_score([d2i[d] for d in valid_d], router.predict(X_va)):.4f}\")\n",
    "for d in DOMAINS:\n",
    "    m = valid_d == d\n",
    "    print(f\"  Probe '{d}' valid acc: {accuracy_score(valid_y[m], probes[d].predict(X_va[m])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning (Î±, Ï„, Î²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fast(alpha, tau, beta, h, y, d_arr):\n",
    "    preds = []\n",
    "    X = scaler.transform(h)\n",
    "    for i in range(len(y)):\n",
    "        xi = X[i:i+1]\n",
    "        dom = i2d[router.predict(xi)[0]]\n",
    "        pt = probes[dom].predict_proba(xi)[0, 1]\n",
    "        gate = 1 if pt >= tau else (-1 if pt <= 1-tau else 0)\n",
    "        preds.append(1 if gate == 1 else 0)\n",
    "    preds = np.array(preds)\n",
    "    return {\n",
    "        \"f1\": f1_score(y, preds, zero_division=0),\n",
    "        \"precision\": precision_score(y, preds, zero_division=0),\n",
    "        \"recall\": recall_score(y, preds, zero_division=0),\n",
    "        \"fpr\": (preds[y==0]==1).mean() if (y==0).sum()>0 else 0,\n",
    "        \"accuracy\": accuracy_score(y, preds),\n",
    "    }\n",
    "\n",
    "print(\"Alpha sweep:\")\n",
    "best_f1, best_alpha = 0, ALPHA\n",
    "for a in [1,2,3,4,5,6,7,8,10,12]:\n",
    "    m = eval_fast(a, TAU, BETA, valid_h[L_STAR], valid_y, valid_d)\n",
    "    print(f\"  a={a:5.1f} | F1={m['f1']:.4f} | Prec={m['precision']:.4f} | Rec={m['recall']:.4f} | FPR={m['fpr']:.4f}\")\n",
    "    if m[\"f1\"] > best_f1: best_f1, best_alpha = m[\"f1\"], a\n",
    "ALPHA = best_alpha; print(f\"  Best alpha = {ALPHA}\")\n",
    "\n",
    "print(\"\\nTau sweep:\")\n",
    "best_f1, best_tau = 0, TAU\n",
    "for t in [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80]:\n",
    "    m = eval_fast(ALPHA, t, BETA, valid_h[L_STAR], valid_y, valid_d)\n",
    "    print(f\"  t={t:.2f} | F1={m['f1']:.4f} | Prec={m['precision']:.4f} | Rec={m['recall']:.4f} | FPR={m['fpr']:.4f}\")\n",
    "    if m[\"f1\"] > best_f1: best_f1, best_tau = m[\"f1\"], t\n",
    "TAU = best_tau; print(f\"  Best tau = {TAU}\")\n",
    "\n",
    "print(\"\\nBeta sweep:\")\n",
    "best_f1, best_beta = 0, BETA\n",
    "for b in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0]:\n",
    "    m = eval_fast(ALPHA, TAU, b, valid_h[L_STAR], valid_y, valid_d)\n",
    "    print(f\"  b={b:.1f} | F1={m['f1']:.4f} | Prec={m['precision']:.4f} | Rec={m['recall']:.4f} | FPR={m['fpr']:.4f}\")\n",
    "    if m[\"f1\"] > best_f1: best_f1, best_beta = m[\"f1\"], b\n",
    "BETA = best_beta; print(f\"  Best beta = {BETA}\")\n",
    "print(f\"\\nFinal: alpha={ALPHA}, tau={TAU}, beta={BETA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation on TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = eval_fast(ALPHA, TAU, BETA, test_h[L_STAR], test_y, test_d)\n",
    "print(\"Overall:\")\n",
    "for k, v in test_m.items(): print(f\"  {k:10s}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Domain:\")\n",
    "for d in DOMAINS:\n",
    "    m = test_d == d\n",
    "    dm = eval_fast(ALPHA, TAU, BETA, test_h[L_STAR][m], test_y[m], test_d[m])\n",
    "    print(f\"  {d:12s} | F1={dm['f1']:.4f} | Prec={dm['precision']:.4f} | Rec={dm['recall']:.4f} | FPR={dm['fpr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generation Demo (Baseline vs ASA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, hook_fn=None, layer=None, max_tokens=256):\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inp = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inp = {k: v.to(device) for k, v in inp.items()}\n",
    "    hook = None\n",
    "    if hook_fn and layer is not None:\n",
    "        hook = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inp, max_new_tokens=max_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        gen = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:], skip_special_tokens=False)\n",
    "    finally:\n",
    "        if hook: hook.remove()\n",
    "    return gen\n",
    "\n",
    "_injected = False\n",
    "_info = {}\n",
    "\n",
    "def asa_hook(module, inp, out):\n",
    "    global _injected, _info\n",
    "    if _injected: return out\n",
    "    h = out[0] if isinstance(out, tuple) else out\n",
    "    rest = out[1:] if isinstance(out, tuple) else None\n",
    "    hl = h[:, -1, :].detach().cpu().float().numpy()\n",
    "    hs = scaler.transform(hl)\n",
    "    dom = i2d[router.predict(hs)[0]]\n",
    "    pt = probes[dom].predict_proba(hs)[0, 1]\n",
    "    gate = 1 if pt >= TAU else (-1 if pt <= 1-TAU else 0)\n",
    "    _info = {\"domain\": dom, \"p_tool\": float(pt), \"gate\": gate}\n",
    "    _injected = True\n",
    "    if gate == 0: return out\n",
    "    vd = domain_vecs[dom]\n",
    "    v = (1-BETA)*vd + BETA*v_global\n",
    "    v = v / (np.linalg.norm(v) + 1e-8)\n",
    "    vt = torch.tensor(v, dtype=torch.float16).to(h.device)\n",
    "    hn = h.clone()\n",
    "    hn[:, -1, :] = h[:, -1, :] + gate * ALPHA * vt\n",
    "    return (hn,)+rest if rest else hn\n",
    "\n",
    "demos = [s for s in test_data if s[\"label\"]==1][:4] + [s for s in test_data if s[\"label\"]==0][:4]\n",
    "for s in demos:\n",
    "    msgs = fmt(s)\n",
    "    bl = generate(msgs)\n",
    "    bl_t = TOOL_S in bl\n",
    "    _injected = False; _info = {}\n",
    "    asa = generate(msgs, hook_fn=asa_hook, layer=L_STAR)\n",
    "    asa_t = TOOL_S in asa\n",
    "    label = \"TOOL\" if s[\"label\"]==1 else \"NO-TOOL\"\n",
    "    gs = {1:\"+1\", -1:\"-1\", 0:\"0\"}.get(_info.get(\"gate\",0), \"?\")\n",
    "    print(f\"[{label}] {s['instruction'][:70]}\")\n",
    "    print(f\"  Baseline: {'triggered' if bl_t else 'no trigger'}\")\n",
    "    print(f\"  ASA:      {'triggered' if asa_t else 'no trigger'} (dom={_info.get('domain','?')}, p={_info.get('p_tool',0):.3f}, gate={gs})\")\n",
    "    print()\n",
    "    if device.type == \"cuda\": torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_dir = OUT_DIR / \"asa_assets\"; assets_dir.mkdir(exist_ok=True)\n",
    "vecs = {\"global\": v_global}; vecs.update(domain_vecs)\n",
    "np.savez(assets_dir / \"steering_vectors.npz\", **vecs)\n",
    "with open(assets_dir / \"router.pkl\", \"wb\") as f: pickle.dump(router, f)\n",
    "with open(assets_dir / \"probes.pkl\", \"wb\") as f: pickle.dump(probes, f)\n",
    "with open(assets_dir / \"scaler.pkl\", \"wb\") as f: pickle.dump(scaler, f)\n",
    "config = {\"model_id\": MODEL_ID, \"best_layer\": int(L_STAR), \"alpha\": ALPHA, \"tau\": TAU, \"beta\": BETA,\n",
    "          \"domains\": DOMAINS, \"test_metrics\": {k: float(v) for k,v in test_m.items()}}\n",
    "with open(assets_dir / \"config.json\", \"w\") as f: json.dump(config, f, indent=2)\n",
    "total_kb = sum(f.stat().st_size for f in assets_dir.iterdir()) / 1024\n",
    "print(f\"Assets saved to {assets_dir} ({total_kb:.0f} KB)\")\n",
    "print(f\"\\nDONE! L*={L_STAR}, alpha={ALPHA}, tau={TAU}, beta={BETA}\")\n",
    "print(f\"Test F1={test_m['f1']:.4f}, FPR={test_m['fpr']:.4f}\")"
   ]
  }
 ]
}