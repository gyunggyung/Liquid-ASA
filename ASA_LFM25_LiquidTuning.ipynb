{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d75bbb",
   "metadata": {},
   "source": [
    "# ASA × LFM2.5 — Liquid Prompt τ/α Tuning\n",
    "\n",
    "**독립 실행 가능** — 이전 노트북 에셋 불필요. 전체 파이프라인을 자체 실행 후 τ/α sweep 수행.\n",
    "\n",
    "**목표**: Liquid 공식 포맷에서 ASA 억제가 너무 강한 문제(Recall 0.63→0.21)를 해결.\n",
    "- 기존: τ=0.50, α=1 → FPR **0.116** (최저) but Recall **0.213** (폭락)\n",
    "- 논문: τ=0.60, α=4 → 더 보수적 게이트 + 더 강한 스티어링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549638b7",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate scikit-learn datasets tqdm matplotlib seaborn\n",
    "\n",
    "import json, re, pickle, os, sys, warnings, gc, ast\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             accuracy_score, confusion_matrix, roc_auc_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"outputs_liquid\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5514a",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "print(f\"Alpaca: {len(ds)} samples\")\n",
    "\n",
    "TOOLS = [\n",
    "    {\"name\": \"calculator\", \"description\": \"Evaluate a mathematical expression and return the numeric result.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}}, \"required\": [\"expression\"]}},\n",
    "    {\"name\": \"python_interpreter\", \"description\": \"Execute Python code and return the output.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"code\": {\"type\": \"string\", \"description\": \"Python source code\"}}, \"required\": [\"code\"]}},\n",
    "    {\"name\": \"web_search\", \"description\": \"Search the web for up-to-date information.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"Search query\"}}, \"required\": [\"query\"]}},\n",
    "    {\"name\": \"translator\", \"description\": \"Translate text from one language to another.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"text\": {\"type\": \"string\", \"description\": \"Text to translate\"}, \"target_language\": {\"type\": \"string\", \"description\": \"Target language\"}}, \"required\": [\"text\", \"target_language\"]}}\n",
    "]\n",
    "\n",
    "SYS_PROMPT = f\"List of tools: {json.dumps(TOOLS)}\"\n",
    "\n",
    "KW = {\n",
    "    \"math\": {\"calculate\", \"compute\", \"solve\", \"equation\", \"sum\", \"average\",\n",
    "             \"percentage\", \"convert\", \"ratio\", \"divide\", \"multiply\"},\n",
    "    \"code\": {\"write a program\", \"code\", \"function\", \"algorithm\", \"implement\",\n",
    "             \"script\", \"debug\", \"compile\", \"class\", \"method\"},\n",
    "    \"search\": {\"search\", \"find\", \"look up\", \"latest\", \"current\", \"news\",\n",
    "               \"recent\", \"who is\", \"what happened\", \"when did\"},\n",
    "    \"translation\": {\"translate\", \"translation\", \"say in\", \"convert to\",\n",
    "                    \"how do you say\", \"in french\", \"in spanish\", \"in german\",\n",
    "                    \"in japanese\", \"in chinese\"},\n",
    "}\n",
    "\n",
    "def classify(text):\n",
    "    t = text.lower()\n",
    "    for domain, keywords in KW.items():\n",
    "        if any(k in t for k in keywords):\n",
    "            return domain\n",
    "    return None\n",
    "\n",
    "np.random.seed(42)\n",
    "buckets = {d: {\"tool\": [], \"non_tool\": []} for d in KW}\n",
    "\n",
    "for i, row in enumerate(ds):\n",
    "    text = row.get(\"instruction\", \"\") + \" \" + row.get(\"input\", \"\")\n",
    "    dom = classify(text)\n",
    "    if dom is None:\n",
    "        continue\n",
    "    output = row.get(\"output\", \"\")\n",
    "    label = \"tool\" if classify(output) == dom or any(k in output.lower() for k in [\"result\", \"output\", \"answer\", \"return\"]) else \"non_tool\"\n",
    "    buckets[dom][label].append(i)\n",
    "\n",
    "for dom in buckets:\n",
    "    for lbl in buckets[dom]:\n",
    "        np.random.shuffle(buckets[dom][lbl])\n",
    "\n",
    "PER_SPLIT_PER_CLASS = 40\n",
    "SPLITS = {\"cal\": 0, \"train\": 1, \"valid\": 2, \"test\": 3}\n",
    "TEST_MULT = 2\n",
    "\n",
    "samples = {s: [] for s in SPLITS}\n",
    "used = set()\n",
    "for dom in buckets:\n",
    "    for lbl in [\"tool\", \"non_tool\"]:\n",
    "        pool = [i for i in buckets[dom][lbl] if i not in used]\n",
    "        for sname, sidx in SPLITS.items():\n",
    "            n = PER_SPLIT_PER_CLASS * (TEST_MULT if sname == \"test\" else 1)\n",
    "            start = sidx * PER_SPLIT_PER_CLASS\n",
    "            end = start + n\n",
    "            chosen = pool[start:end]\n",
    "            used.update(chosen)\n",
    "            is_tool = 1 if lbl == \"tool\" else 0\n",
    "            for idx in chosen:\n",
    "                row = ds[int(idx)]\n",
    "                text = row.get(\"instruction\", \"\") + \" \" + row.get(\"input\", \"\")\n",
    "                samples[sname].append({\"text\": text, \"label\": is_tool, \"domain\": dom, \"idx\": int(idx)})\n",
    "\n",
    "for s in samples:\n",
    "    np.random.shuffle(samples[s])\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "for s in samples:\n",
    "    t = sum(1 for x in samples[s] if x[\"label\"] == 1)\n",
    "    nt = len(samples[s]) - t\n",
    "    print(f\"  {s:5s}: {len(samples[s])} ({t} tool / {nt} non-tool)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141f7d4",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "TOOL_S = \"<|tool_call_start|>\"\n",
    "TOOL_E = \"<|tool_call_end|>\"\n",
    "NUM_LAYERS = model.config.num_hidden_layers\n",
    "print(f\"{MODEL_ID} ({sum(p.numel() for p in model.parameters()) // 1_000_000}M params, {NUM_LAYERS} layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e331c",
   "metadata": {},
   "source": [
    "## 4. Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(split_data, split_name):\n",
    "    '''Extract last-token hidden states from all layers.'''\n",
    "    all_hs = {l: [] for l in range(NUM_LAYERS)}\n",
    "    labels, domains = [], []\n",
    "    hooks, captured = [], {}\n",
    "\n",
    "    def make_hook(layer_idx):\n",
    "        def hook_fn(module, inp, out):\n",
    "            h = out[0] if isinstance(out, tuple) else out\n",
    "            captured[layer_idx] = h[:, -1, :].detach().cpu().float().numpy()\n",
    "        return hook_fn\n",
    "\n",
    "    for l in range(NUM_LAYERS):\n",
    "        hooks.append(model.model.layers[l].register_forward_hook(make_hook(l)))\n",
    "\n",
    "    for sample in tqdm(split_data, desc=split_name):\n",
    "        msgs = [{\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": sample[\"text\"]}]\n",
    "        text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        captured.clear()\n",
    "        with torch.no_grad():\n",
    "            model(ids)\n",
    "        for l in range(NUM_LAYERS):\n",
    "            all_hs[l].append(captured[l].squeeze(0))\n",
    "        labels.append(sample[\"label\"])\n",
    "        domains.append(sample[\"domain\"])\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return {l: np.stack(all_hs[l]) for l in range(NUM_LAYERS)}, np.array(labels), domains\n",
    "\n",
    "hs = {}\n",
    "labels_dict = {}\n",
    "domains_dict = {}\n",
    "for split_name in [\"cal\", \"train\", \"valid\", \"test\"]:\n",
    "    hs[split_name], labels_dict[split_name], domains_dict[split_name] = \\\n",
    "        extract_hidden_states(samples[split_name], split_name)\n",
    "\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "print(\"Hidden states extracted for all splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daecd76f",
   "metadata": {},
   "source": [
    "## 5. Probe Sweep → Find L*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fdef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probe sweep across all layers:\\n\")\n",
    "best_auc, best_layer, plateau = 0, 0, 0\n",
    "\n",
    "for l in range(NUM_LAYERS):\n",
    "    scaler_tmp = StandardScaler().fit(hs[\"cal\"][l])\n",
    "    X = scaler_tmp.transform(hs[\"cal\"][l])\n",
    "    y = labels_dict[\"cal\"]\n",
    "    probe = LogisticRegression(max_iter=2000, C=1.0).fit(X, y)\n",
    "    X_v = scaler_tmp.transform(hs[\"valid\"][l])\n",
    "    y_v = labels_dict[\"valid\"]\n",
    "    probs = probe.predict_proba(X_v)[:, 1]\n",
    "    auc = roc_auc_score(y_v, probs)\n",
    "    acc = probe.score(X_v, y_v)\n",
    "    print(f\"  Layer {l:2d} | AUC: {auc:.4f} | Acc: {acc:.4f}\")\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_layer = l\n",
    "        plateau = 0\n",
    "    else:\n",
    "        plateau += 1\n",
    "\n",
    "L_STAR = best_layer\n",
    "print(f\"\\nL* = {L_STAR} (AUC={best_auc:.4f}, plateau={plateau} layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580e756",
   "metadata": {},
   "source": [
    "## 6. Build Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = sorted(set(domains_dict[\"cal\"]))\n",
    "h_cal = hs[\"cal\"][L_STAR]\n",
    "y_cal = labels_dict[\"cal\"]\n",
    "d_cal = domains_dict[\"cal\"]\n",
    "\n",
    "steering_vecs = {}\n",
    "# Global vector\n",
    "tool_mask = y_cal == 1\n",
    "nontool_mask = y_cal == 0\n",
    "v_global = h_cal[tool_mask].mean(axis=0) - h_cal[nontool_mask].mean(axis=0)\n",
    "steering_vecs[\"global\"] = v_global / (np.linalg.norm(v_global) + 1e-8)\n",
    "\n",
    "# Domain vectors\n",
    "for dom in DOMAINS:\n",
    "    dom_mask = np.array([d == dom for d in d_cal])\n",
    "    tool_dom = dom_mask & tool_mask\n",
    "    nontool_dom = dom_mask & nontool_mask\n",
    "    if tool_dom.sum() > 0 and nontool_dom.sum() > 0:\n",
    "        v_d = h_cal[tool_dom].mean(axis=0) - h_cal[nontool_dom].mean(axis=0)\n",
    "        steering_vecs[dom] = v_d / (np.linalg.norm(v_d) + 1e-8)\n",
    "        cos = np.dot(steering_vecs[dom], steering_vecs[\"global\"])\n",
    "        print(f\"  {dom:12s} cos(v_d, v_g) = {cos:.4f}\")\n",
    "\n",
    "print(\"Vectors built from CAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861189bc",
   "metadata": {},
   "source": [
    "## 7. Train Router & Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30711819",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(hs[\"cal\"][L_STAR])\n",
    "\n",
    "# Router (4-class domain classifier)\n",
    "X_train_r = scaler.transform(hs[\"train\"][L_STAR])\n",
    "y_train_r = np.array([DOMAINS.index(d) for d in domains_dict[\"train\"]])\n",
    "router = LogisticRegression(max_iter=2000, C=1.0, multi_class=\"multinomial\").fit(X_train_r, y_train_r)\n",
    "\n",
    "# Per-domain probes\n",
    "probes = {}\n",
    "for dom in DOMAINS:\n",
    "    mask = np.array([d == dom for d in domains_dict[\"train\"]])\n",
    "    if mask.sum() < 4:\n",
    "        continue\n",
    "    X_d = scaler.transform(hs[\"train\"][L_STAR][mask])\n",
    "    y_d = labels_dict[\"train\"][mask]\n",
    "    probes[dom] = LogisticRegression(max_iter=2000, C=1.0).fit(X_d, y_d)\n",
    "    print(f\"  Probe '{dom}' train acc: {probes[dom].score(X_d, y_d):.4f}\")\n",
    "\n",
    "# Validation\n",
    "X_val = scaler.transform(hs[\"valid\"][L_STAR])\n",
    "y_val_r = np.array([DOMAINS.index(d) for d in domains_dict[\"valid\"]])\n",
    "print(f\"  Router valid acc: {router.score(X_val, y_val_r):.4f}\")\n",
    "\n",
    "for dom in DOMAINS:\n",
    "    mask = np.array([d == dom for d in domains_dict[\"valid\"]])\n",
    "    if dom in probes and mask.sum() > 0:\n",
    "        X_d = scaler.transform(hs[\"valid\"][L_STAR][mask])\n",
    "        y_d = labels_dict[\"valid\"][mask]\n",
    "        print(f\"  Probe '{dom}' valid acc: {probes[dom].score(X_d, y_d):.4f}\")\n",
    "\n",
    "print(\"Router & probes trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d915a",
   "metadata": {},
   "source": [
    "## 8. Generation & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, hook_fn=None, layer=None, max_new=128):\n",
    "    '''Generate text, optionally with ASA hook.'''\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    hook = None\n",
    "    if hook_fn and layer is not None:\n",
    "        hook = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(ids, max_new_tokens=max_new, do_sample=False,\n",
    "                             pad_token_id=tokenizer.eos_token_id)\n",
    "    if hook:\n",
    "        hook.remove()\n",
    "    return tokenizer.decode(out[0][ids.shape[1]:], skip_special_tokens=False)\n",
    "\n",
    "def evaluate_split(split_data, alpha, tau, beta, use_asa=True):\n",
    "    '''Evaluate on TEST with given hyperparameters.'''\n",
    "    global _injected\n",
    "\n",
    "    def get_steer_vec(domain):\n",
    "        v_d = steering_vecs.get(domain)\n",
    "        v_g = steering_vecs.get(\"global\")\n",
    "        if v_d is None:\n",
    "            return v_g\n",
    "        if v_g is not None and beta > 0:\n",
    "            v = beta * v_g + (1 - beta) * v_d\n",
    "        else:\n",
    "            v = v_d\n",
    "        return v / (np.linalg.norm(v) + 1e-8)\n",
    "\n",
    "    def asa_hook(module, inp, out):\n",
    "        global _injected\n",
    "        if _injected:\n",
    "            return out\n",
    "        _injected = True\n",
    "        h = out[0] if isinstance(out, tuple) else out\n",
    "        hl = h[:, -1, :].detach().cpu().float().numpy()\n",
    "        hs_scaled = scaler.transform(hl)\n",
    "        dom = DOMAINS[router.predict(hs_scaled)[0]]\n",
    "        pt = probes[dom].predict_proba(hs_scaled)[0, 1] if dom in probes else 0.5\n",
    "        gate = 1 if pt >= tau else (-1 if pt <= 1 - tau else 0)\n",
    "        if gate == 0:\n",
    "            return out\n",
    "        v = get_steer_vec(dom)\n",
    "        vt = torch.tensor(v, dtype=torch.float16).to(h.device)\n",
    "        hn = h.clone()\n",
    "        hn[:, -1, :] += gate * alpha * vt\n",
    "        rest = out[1:] if isinstance(out, tuple) else None\n",
    "        return (hn,) + rest if rest else hn\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    domain_results = {}\n",
    "\n",
    "    for sample in tqdm(split_data, desc=f\"a={alpha} t={tau}\", leave=False):\n",
    "        msgs = [{\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": sample[\"text\"]}]\n",
    "        if use_asa:\n",
    "            _injected = False\n",
    "            out = generate(msgs, hook_fn=asa_hook, layer=L_STAR)\n",
    "        else:\n",
    "            out = generate(msgs)\n",
    "        triggered = 1 if TOOL_S in out else 0\n",
    "        y_true.append(sample[\"label\"])\n",
    "        y_pred.append(triggered)\n",
    "        domain = sample[\"domain\"]\n",
    "        if domain not in domain_results:\n",
    "            domain_results[domain] = {\"y_true\": [], \"y_pred\": []}\n",
    "        domain_results[domain][\"y_true\"].append(sample[\"label\"])\n",
    "        domain_results[domain][\"y_pred\"].append(triggered)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    results = {\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"fpr\": fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "    for dom, dr in domain_results.items():\n",
    "        tn_d, fp_d, fn_d, tp_d = confusion_matrix(dr[\"y_true\"], dr[\"y_pred\"], labels=[0, 1]).ravel()\n",
    "        results[f\"{dom}_f1\"] = f1_score(dr[\"y_true\"], dr[\"y_pred\"], zero_division=0)\n",
    "        results[f\"{dom}_precision\"] = precision_score(dr[\"y_true\"], dr[\"y_pred\"], zero_division=0)\n",
    "        results[f\"{dom}_recall\"] = recall_score(dr[\"y_true\"], dr[\"y_pred\"], zero_division=0)\n",
    "        results[f\"{dom}_fpr\"] = fp_d / (fp_d + tn_d) if (fp_d + tn_d) > 0 else 0\n",
    "    return results\n",
    "\n",
    "_injected = False\n",
    "print(\"Functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf32317",
   "metadata": {},
   "source": [
    "## 9. Baseline (No ASA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Baseline (no ASA) on TEST set...\")\n",
    "baseline_results = evaluate_split(samples[\"test\"], alpha=0, tau=0, beta=0, use_asa=False)\n",
    "print(f\"\\n  Baseline: F1={baseline_results['f1']:.4f}  FPR={baseline_results['fpr']:.4f}  \"\n",
    "      f\"Rec={baseline_results['recall']:.4f}  Prec={baseline_results['precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e4e7f",
   "metadata": {},
   "source": [
    "## 10. τ/α Grid Sweep (핵심 실험)\n",
    "\n",
    "> **논문**: Qwen2.5-1.5B에서 α=4, τ=0.60 사용.\n",
    "> 기존 Liquid 결과: α=1, τ=0.50 → FPR 0.116 but Recall 0.213 (과잉 억제)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = [\n",
    "    (\"current\",      0.50, 1, 0.0),\n",
    "    (\"paper\",        0.60, 4, 0.0),\n",
    "    (\"gentle\",       0.70, 2, 0.0),\n",
    "    (\"balanced\",     0.65, 3, 0.0),\n",
    "    (\"conservative\", 0.70, 4, 0.0),\n",
    "    (\"selective\",    0.75, 5, 0.0),\n",
    "    (\"paper_high\",   0.60, 8, 0.0),\n",
    "    (\"minimal\",      0.80, 3, 0.0),\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for name, tau, alpha, beta in EXPERIMENTS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {name} (tau={tau}, alpha={alpha})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results = evaluate_split(samples[\"test\"], alpha=alpha, tau=tau, beta=beta, use_asa=True)\n",
    "    all_results[name] = {\"tau\": tau, \"alpha\": alpha, \"beta\": beta, **results}\n",
    "    print(f\"  F1={results['f1']:.4f}  Prec={results['precision']:.4f}  \"\n",
    "          f\"Rec={results['recall']:.4f}  FPR={results['fpr']:.4f}\")\n",
    "\n",
    "print(\"\\n\\nAll experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fe198",
   "metadata": {},
   "source": [
    "## 11. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6861bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_BEST = {\"f1\": 0.6185, \"precision\": 0.7591, \"recall\": 0.5219,\n",
    "             \"fpr\": 0.1656, \"accuracy\": 0.6781}\n",
    "\n",
    "sorted_r = sorted(all_results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Name':>14s} | {'tau':>4s} | {'a':>3s} | {'F1':>6s} | {'Prec':>6s} | {'Rec':>6s} | {'FPR':>6s} | vs Qwen F1\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'baseline':>14s} | {'--':>4s} | {'--':>3s} | {baseline_results['f1']:>6.4f} | \"\n",
    "      f\"{baseline_results['precision']:>6.4f} | {baseline_results['recall']:>6.4f} | \"\n",
    "      f\"{baseline_results['fpr']:>6.4f} | --\")\n",
    "print(f\"{'Qwen+ASA ref':>14s} | {'--':>4s} | {'--':>3s} | {QWEN_BEST['f1']:>6.4f} | \"\n",
    "      f\"{QWEN_BEST['precision']:>6.4f} | {QWEN_BEST['recall']:>6.4f} | \"\n",
    "      f\"{QWEN_BEST['fpr']:>6.4f} | reference\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name, r in sorted_r:\n",
    "    delta = r['f1'] - QWEN_BEST['f1']\n",
    "    mark = \"BEATS!\" if delta > 0 else f\"{delta:+.4f}\"\n",
    "    star = \" ***\" if r['f1'] == max(v['f1'] for v in all_results.values()) else \"\"\n",
    "    print(f\"{name:>14s} | {r['tau']:>4.2f} | {r['alpha']:>3d} | {r['f1']:>6.4f} | \"\n",
    "          f\"{r['precision']:>6.4f} | {r['recall']:>6.4f} | {r['fpr']:>6.4f} | {mark}{star}\")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "\n",
    "best_f1_name = max(all_results, key=lambda k: all_results[k]['f1'])\n",
    "best_fpr_name = min(all_results, key=lambda k: all_results[k]['fpr'])\n",
    "print(f\"\\nBest F1:  {best_f1_name} -> F1={all_results[best_f1_name]['f1']:.4f}\")\n",
    "print(f\"Best FPR: {best_fpr_name} -> FPR={all_results[best_fpr_name]['fpr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577705f2",
   "metadata": {},
   "source": [
    "## 12. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99398c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "names = [n for n, _ in sorted_r]\n",
    "f1s = [r['f1'] for _, r in sorted_r]\n",
    "fprs = [r['fpr'] for _, r in sorted_r]\n",
    "recs = [r['recall'] for _, r in sorted_r]\n",
    "\n",
    "for ax, vals, xlabel, title, ref_val in zip(\n",
    "    axes, [f1s, fprs, recs],\n",
    "    ['F1', 'FPR (lower=better)', 'Recall'],\n",
    "    ['F1 Score', 'False Positive Rate', 'Recall'],\n",
    "    [QWEN_BEST['f1'], QWEN_BEST['fpr'], QWEN_BEST['recall']]\n",
    "):\n",
    "    colors = ['#2ecc71' if (v > ref_val if xlabel != 'FPR (lower=better)' else v < ref_val)\n",
    "              else '#e74c3c' for v in vals]\n",
    "    ax.barh(names, vals, color=colors)\n",
    "    ax.axvline(ref_val, color='orange', linestyle='--', label=f'Qwen+ASA ({ref_val:.3f})')\n",
    "    ax.axvline(baseline_results.get(xlabel.split()[0].lower(), 0), color='gray',\n",
    "               linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('ASA Liquid Prompt - tau/alpha Tuning Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs_liquid/tuning_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45a6bd",
   "metadata": {},
   "source": [
    "## 13. FPR vs Recall Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for name, r in all_results.items():\n",
    "    ax.scatter(r['fpr'], r['recall'], s=100, zorder=5)\n",
    "    ax.annotate(f\"{name}\\nt={r['tau']},a={r['alpha']}\\nF1={r['f1']:.3f}\",\n",
    "                (r['fpr'], r['recall']), textcoords=\"offset points\",\n",
    "                xytext=(10, 5), fontsize=8)\n",
    "\n",
    "ax.scatter(baseline_results['fpr'], baseline_results['recall'], s=200, marker='X',\n",
    "           color='gray', zorder=6, label='Baseline (no ASA)')\n",
    "ax.scatter(QWEN_BEST['fpr'], QWEN_BEST['recall'], s=200, marker='D',\n",
    "           color='orange', zorder=6, label='Qwen+ASA (best)')\n",
    "\n",
    "ax.set_xlabel('FPR (lower = better)', fontsize=12)\n",
    "ax.set_ylabel('Recall (higher = better)', fontsize=12)\n",
    "ax.set_title('FPR vs Recall Tradeoff', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs_liquid/pareto_front.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245692fb",
   "metadata": {},
   "source": [
    "## 14. Domain Analysis (Best Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc339ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(all_results, key=lambda k: all_results[k]['f1'])\n",
    "best = all_results[best_name]\n",
    "print(f\"Best: {best_name} (tau={best['tau']}, alpha={best['alpha']})\\n\")\n",
    "for dom in sorted(set(domains_dict[\"test\"])):\n",
    "    f1 = best.get(f\"{dom}_f1\", 0)\n",
    "    p = best.get(f\"{dom}_precision\", 0)\n",
    "    r = best.get(f\"{dom}_recall\", 0)\n",
    "    fpr = best.get(f\"{dom}_fpr\", 0)\n",
    "    print(f\"  {dom:>12s} | F1={f1:.4f} | P={p:.4f} | R={r:.4f} | FPR={fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7735f",
   "metadata": {},
   "source": [
    "## 15. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527220f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(all_results, key=lambda k: all_results[k]['f1'])\n",
    "best_fpr_name = min(all_results, key=lambda k: all_results[k]['fpr'])\n",
    "\n",
    "save_data = {\n",
    "    \"baseline\": baseline_results,\n",
    "    \"qwen_style_reference\": QWEN_BEST,\n",
    "    \"experiments\": all_results,\n",
    "    \"best_f1_config\": best_name,\n",
    "    \"best_fpr_config\": best_fpr_name,\n",
    "    \"L_star\": L_STAR,\n",
    "    \"domains\": DOMAINS,\n",
    "}\n",
    "with open(\"outputs_liquid/tuning_results.json\", \"w\") as f:\n",
    "    json.dump(save_data, f, indent=2, default=str)\n",
    "\n",
    "# Also save assets for future use\n",
    "asset_dir = Path(\"outputs_liquid/asa_assets\")\n",
    "asset_dir.mkdir(parents=True, exist_ok=True)\n",
    "np.savez(asset_dir / \"steering_vectors.npz\", **steering_vecs)\n",
    "pickle.dump(router, open(asset_dir / \"router.pkl\", \"wb\"))\n",
    "pickle.dump(probes, open(asset_dir / \"probes.pkl\", \"wb\"))\n",
    "pickle.dump(scaler, open(asset_dir / \"scaler.pkl\", \"wb\"))\n",
    "\n",
    "best_r = all_results[best_name]\n",
    "config_save = {\"L_star\": L_STAR, \"alpha\": best_r[\"alpha\"], \"tau\": best_r[\"tau\"],\n",
    "               \"beta\": best_r[\"beta\"], \"domains\": DOMAINS}\n",
    "json.dump(config_save, open(asset_dir / \"config.json\", \"w\"), indent=2)\n",
    "\n",
    "total_kb = sum(f.stat().st_size for f in asset_dir.iterdir()) / 1024\n",
    "print(f\"Assets saved: {asset_dir} ({total_kb:.0f} KB)\")\n",
    "print(f\"Results saved: outputs_liquid/tuning_results.json\")\n",
    "print(f\"\\nBest F1: {best_name} (tau={best_r['tau']}, alpha={best_r['alpha']}) -> F1={best_r['f1']:.4f}\")\n",
    "print(f\"Best FPR: {best_fpr_name} -> FPR={all_results[best_fpr_name]['fpr']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
