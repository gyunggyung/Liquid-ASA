{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254d1436",
   "metadata": {},
   "source": [
    "# ASA Ã— LFM2.5 â€” Liquid Prompt Ï„/Î± Tuning\n",
    "\n",
    "**ëª©í‘œ**: Liquid ê³µì‹ í¬ë§·ì—ì„œ ASA ì–µì œê°€ ë„ˆë¬´ ê°•í•œ ë¬¸ì œ(Recall 0.63â†’0.21)ë¥¼ í•´ê²°.\n",
    "- ê¸°ì¡´: Ï„=0.50, Î±=1 â†’ FPR **0.116** (ìµœì €) but Recall **0.213** (í­ë½)\n",
    "- ë…¼ë¬¸: Ï„=0.60, Î±=4 â†’ ë” ë³´ìˆ˜ì  ê²Œì´íŠ¸ + ë” ê°•í•œ ìŠ¤í‹°ì–´ë§\n",
    "\n",
    "**í•µì‹¬ ì•„ì´ë””ì–´**: Ï„ë¥¼ ë†’ì´ë©´ \"í™•ì‹¤í•  ë•Œë§Œ\" ê°œì…í•˜ê³ , Î±ë¥¼ í‚¤ìš°ë©´ ê°œì… ì‹œ ë” íš¨ê³¼ì .\n",
    "\n",
    "## ì‹¤í—˜ ë§¤íŠ¸ë¦­ìŠ¤\n",
    "\n",
    "| ì‹¤í—˜ | Ï„ | Î± | ì „ëµ |\n",
    "|------|---|---|------|\n",
    "| baseline | 0.50 | 1 | í˜„ì¬ (ë¹„êµ ê¸°ì¤€) |\n",
    "| paper | **0.60** | **4** | ë…¼ë¬¸ ì„¤ì • ê·¸ëŒ€ë¡œ |\n",
    "| conservative | **0.70** | 3 | ë³´ìˆ˜ì  ê²Œì´íŠ¸ + ì¤‘ê°„ ê°•ë„ |\n",
    "| balanced | **0.65** | 5 | ì ë‹¹í•œ ê²Œì´íŠ¸ + ê°•í•œ ì´‰ì§„ |\n",
    "| gentle | **0.75** | 2 | ì•½í•œ ê°œì… |\n",
    "| aggressive | 0.55 | 8 | ì•½ê°„ë§Œ ì™„í™” + ë§¤ìš° ê°•í•œ ì´‰ì§„ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e22e2",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b502ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate scikit-learn datasets tqdm matplotlib seaborn\n",
    "\n",
    "import json, re, pickle, os, sys, warnings, gc, ast\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             accuracy_score, confusion_matrix)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e323e",
   "metadata": {},
   "source": [
    "## 2. Load Data (Same Pipeline as LiquidPrompt notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074bda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "print(f\"Alpaca: {len(ds)} samples\")\n",
    "\n",
    "# Tool definitions\n",
    "TOOLS = [\n",
    "    {\"name\": \"calculator\", \"description\": \"Evaluate a mathematical expression and return the numeric result.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}}, \"required\": [\"expression\"]}},\n",
    "    {\"name\": \"python_interpreter\", \"description\": \"Execute Python code and return the output.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"code\": {\"type\": \"string\", \"description\": \"Python source code\"}}, \"required\": [\"code\"]}},\n",
    "    {\"name\": \"web_search\", \"description\": \"Search the web for up-to-date information.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"Search query\"}}, \"required\": [\"query\"]}},\n",
    "    {\"name\": \"translator\", \"description\": \"Translate text from one language to another.\",\n",
    "     \"parameters\": {\"type\": \"object\", \"properties\": {\"text\": {\"type\": \"string\", \"description\": \"Text to translate\"}, \"target_language\": {\"type\": \"string\", \"description\": \"Target language\"}}, \"required\": [\"text\", \"target_language\"]}}\n",
    "]\n",
    "\n",
    "# Liquid official system prompt\n",
    "SYS_PROMPT = f\"List of tools: {json.dumps(TOOLS)}\"\n",
    "\n",
    "# Keyword filters\n",
    "KW = {\n",
    "    \"math\": {\"calculate\", \"compute\", \"solve\", \"equation\", \"sum\", \"average\",\n",
    "             \"percentage\", \"convert\", \"ratio\", \"divide\", \"multiply\"},\n",
    "    \"code\": {\"write a program\", \"code\", \"function\", \"algorithm\", \"implement\",\n",
    "             \"script\", \"debug\", \"compile\", \"class\", \"method\"},\n",
    "    \"search\": {\"search\", \"find\", \"look up\", \"latest\", \"current\", \"news\",\n",
    "               \"recent\", \"who is\", \"what happened\", \"when did\"},\n",
    "    \"translation\": {\"translate\", \"translation\", \"say in\", \"convert to\",\n",
    "                    \"how do you say\", \"in french\", \"in spanish\", \"in german\",\n",
    "                    \"in japanese\", \"in chinese\"},\n",
    "}\n",
    "\n",
    "def classify(text):\n",
    "    t = text.lower()\n",
    "    for domain, keywords in KW.items():\n",
    "        if any(k in t for k in keywords):\n",
    "            return domain\n",
    "    return None\n",
    "\n",
    "np.random.seed(42)\n",
    "buckets = {\"math\": {\"tool\": [], \"non_tool\": []}, \"code\": {\"tool\": [], \"non_tool\": []},\n",
    "           \"search\": {\"tool\": [], \"non_tool\": []}, \"translation\": {\"tool\": [], \"non_tool\": []}}\n",
    "\n",
    "for i, row in enumerate(ds):\n",
    "    text = row.get(\"instruction\", \"\") + \" \" + row.get(\"input\", \"\")\n",
    "    dom = classify(text)\n",
    "    if dom is None:\n",
    "        continue\n",
    "    output = row.get(\"output\", \"\")\n",
    "    label = \"tool\" if classify(output) == dom or any(k in output.lower() for k in [\"result\", \"output\", \"answer\", \"return\"]) else \"non_tool\"\n",
    "    buckets[dom][label].append(i)\n",
    "\n",
    "for dom in buckets:\n",
    "    for lbl in buckets[dom]:\n",
    "        np.random.shuffle(buckets[dom][lbl])\n",
    "\n",
    "PER_SPLIT_PER_CLASS = 40\n",
    "SPLITS = {\"cal\": 0, \"train\": 1, \"valid\": 2, \"test\": 3}\n",
    "TEST_MULT = 2\n",
    "\n",
    "samples = {s: [] for s in SPLITS}\n",
    "used = set()\n",
    "for dom in buckets:\n",
    "    for lbl in [\"tool\", \"non_tool\"]:\n",
    "        pool = [i for i in buckets[dom][lbl] if i not in used]\n",
    "        for sname, sidx in SPLITS.items():\n",
    "            n = PER_SPLIT_PER_CLASS * (TEST_MULT if sname == \"test\" else 1)\n",
    "            start = sidx * PER_SPLIT_PER_CLASS\n",
    "            end = start + n\n",
    "            chosen = pool[start:end]\n",
    "            used.update(chosen)\n",
    "            is_tool = 1 if lbl == \"tool\" else 0\n",
    "            for idx in chosen:\n",
    "                row = ds[int(idx)]\n",
    "                text = row.get(\"instruction\", \"\") + \" \" + row.get(\"input\", \"\")\n",
    "                samples[sname].append({\"text\": text, \"label\": is_tool, \"domain\": dom, \"idx\": int(idx)})\n",
    "\n",
    "for s in samples:\n",
    "    np.random.shuffle(samples[s])\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "for s in samples:\n",
    "    t = sum(1 for x in samples[s] if x[\"label\"] == 1)\n",
    "    nt = len(samples[s]) - t\n",
    "    print(f\"  {s:5s}: {len(samples[s])} ({t} tool / {nt} non-tool)\")\n",
    "print(f\"Total: {sum(len(v) for v in samples.values())} unique samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81215968",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c70302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "TOOL_S = \"<|tool_call_start|>\"\n",
    "TOOL_E = \"<|tool_call_end|>\"\n",
    "NUM_LAYERS = model.config.num_hidden_layers\n",
    "print(f\"{MODEL_ID} ({sum(p.numel() for p in model.parameters()) // 1_000_000}M params, {NUM_LAYERS} layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e91c92",
   "metadata": {},
   "source": [
    "## 4. Load Saved Assets from LiquidPrompt Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if assets exist from previous run, otherwise extract fresh\n",
    "ASSET_DIR = Path(\"outputs_liquid/asa_assets\")\n",
    "\n",
    "if ASSET_DIR.exists():\n",
    "    print(\"Loading saved assets from outputs_liquid/asa_assets/\")\n",
    "    import pickle\n",
    "    router = pickle.load(open(ASSET_DIR / \"router.pkl\", \"rb\"))\n",
    "    probes = pickle.load(open(ASSET_DIR / \"probes.pkl\", \"rb\"))\n",
    "    scaler = pickle.load(open(ASSET_DIR / \"scaler.pkl\", \"rb\"))\n",
    "    config = json.load(open(ASSET_DIR / \"config.json\"))\n",
    "    vecs_npz = np.load(ASSET_DIR / \"steering_vectors.npz\")\n",
    "    steering_vecs = {k: vecs_npz[k] for k in vecs_npz.files}\n",
    "    L_STAR = config[\"L_star\"]\n",
    "    DOMAINS = config[\"domains\"]\n",
    "    print(f\"  L* = {L_STAR}, domains = {DOMAINS}\")\n",
    "    print(f\"  Original: alpha={config['alpha']}, tau={config['tau']}, beta={config['beta']}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"outputs_liquid/asa_assets/ not found! \"\n",
    "        \"Run ASA_LFM25_LiquidPrompt.ipynb first to generate assets.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0825ba8",
   "metadata": {},
   "source": [
    "## 5. Define Generation & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, hook_fn=None, layer=None, max_new=128):\n",
    "    '''Generate text, optionally with ASA hook.'''\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    hook = None\n",
    "    if hook_fn and layer is not None:\n",
    "        hook = model.model.layers[layer].register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(ids, max_new_tokens=max_new, do_sample=False,\n",
    "                             pad_token_id=tokenizer.eos_token_id)\n",
    "    if hook:\n",
    "        hook.remove()\n",
    "    return tokenizer.decode(out[0][ids.shape[1]:], skip_special_tokens=False)\n",
    "\n",
    "def evaluate_split(split_data, alpha, tau, beta, use_asa=True):\n",
    "    '''Evaluate a split with given hyperparameters.'''\n",
    "    global _injected\n",
    "\n",
    "    # Build mixed steering vector based on beta\n",
    "    def get_steer_vec(domain):\n",
    "        v_d = steering_vecs.get(domain)\n",
    "        v_g = steering_vecs.get(\"global\")\n",
    "        if v_d is None:\n",
    "            return v_g\n",
    "        if v_g is not None and beta > 0:\n",
    "            v = beta * v_g + (1 - beta) * v_d\n",
    "        else:\n",
    "            v = v_d\n",
    "        return v / (np.linalg.norm(v) + 1e-8)\n",
    "\n",
    "    def asa_hook(module, inp, out):\n",
    "        global _injected\n",
    "        if _injected:\n",
    "            return out\n",
    "        _injected = True\n",
    "        h = out[0] if isinstance(out, tuple) else out\n",
    "        hl = h[:, -1, :].detach().cpu().float().numpy()\n",
    "        hs = scaler.transform(hl)\n",
    "        dom = DOMAINS[router.predict(hs)[0]]\n",
    "        pt = probes[dom].predict_proba(hs)[0, 1] if dom in probes else 0.5\n",
    "        gate = 1 if pt >= tau else (-1 if pt <= 1 - tau else 0)\n",
    "        if gate == 0:\n",
    "            return out\n",
    "        v = get_steer_vec(dom)\n",
    "        vt = torch.tensor(v, dtype=torch.float16).to(h.device)\n",
    "        hn = h.clone()\n",
    "        hn[:, -1, :] += gate * alpha * vt\n",
    "        rest = out[1:] if isinstance(out, tuple) else None\n",
    "        return (hn,) + rest if rest else hn\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    json_ok, schema_ok, args_ok = 0, 0, 0\n",
    "    n_triggered = 0\n",
    "    domain_results = {}\n",
    "\n",
    "    for sample in tqdm(split_data, desc=f\"Eval a={alpha} t={tau}\", leave=False):\n",
    "        text = sample[\"text\"]\n",
    "        label = sample[\"label\"]\n",
    "        domain = sample[\"domain\"]\n",
    "        msgs = [{\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "        if use_asa:\n",
    "            _injected = False\n",
    "            out = generate(msgs, hook_fn=asa_hook, layer=L_STAR)\n",
    "        else:\n",
    "            out = generate(msgs)\n",
    "\n",
    "        triggered = 1 if TOOL_S in out else 0\n",
    "        y_true.append(label)\n",
    "        y_pred.append(triggered)\n",
    "\n",
    "        if triggered:\n",
    "            n_triggered += 1\n",
    "            tc = out.split(TOOL_S)[-1].split(TOOL_E)[0].strip() if TOOL_E in out else \"\"\n",
    "            try:\n",
    "                parsed = json.loads(tc)\n",
    "                json_ok += 1\n",
    "                if isinstance(parsed, dict) and \"name\" in parsed:\n",
    "                    schema_ok += 1\n",
    "                    if \"arguments\" in parsed or \"parameters\" in parsed:\n",
    "                        args_ok += 1\n",
    "            except:\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(tc)\n",
    "                    if isinstance(parsed, (dict, list)):\n",
    "                        json_ok += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        if domain not in domain_results:\n",
    "            domain_results[domain] = {\"y_true\": [], \"y_pred\": []}\n",
    "        domain_results[domain][\"y_true\"].append(label)\n",
    "        domain_results[domain][\"y_pred\"].append(triggered)\n",
    "\n",
    "    # Overall metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    results = {\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"fpr\": fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"json_valid\": json_ok / n_triggered if n_triggered > 0 else 0,\n",
    "        \"schema_ok\": schema_ok / n_triggered if n_triggered > 0 else 0,\n",
    "        \"args_ok\": args_ok / n_triggered if n_triggered > 0 else 0,\n",
    "    }\n",
    "\n",
    "    # Per-domain metrics\n",
    "    for dom, dr in domain_results.items():\n",
    "        tn_d, fp_d, fn_d, tp_d = confusion_matrix(dr[\"y_true\"], dr[\"y_pred\"], labels=[0, 1]).ravel()\n",
    "        results[f\"{dom}_f1\"] = f1_score(dr[\"y_true\"], dr[\"y_pred\"], zero_division=0)\n",
    "        results[f\"{dom}_precision\"] = precision_score(dr[\"y_true\"], dr[\"y_pred\"], zero_division=0)\n",
    "        results[f\"{dom}_recall\"] = recall_score(dr[\"y_true\"], dr[\"y_pred\"], zero_division=0)\n",
    "        results[f\"{dom}_fpr\"] = fp_d / (fp_d + tn_d) if (fp_d + tn_d) > 0 else 0\n",
    "\n",
    "    return results\n",
    "\n",
    "# Verify hook works\n",
    "_injected = False\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd6103",
   "metadata": {},
   "source": [
    "## 6. Baseline (No ASA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Baseline (no ASA) on TEST set...\")\n",
    "baseline_results = evaluate_split(samples[\"test\"], alpha=0, tau=0, beta=0, use_asa=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[Baseline - Liquid Prompt, No ASA]\")\n",
    "print(f\"  Trig P/R/F1: {baseline_results['precision']:.4f} / {baseline_results['recall']:.4f} / {baseline_results['f1']:.4f}\")\n",
    "print(f\"  FPR: {baseline_results['fpr']:.4f}  Acc: {baseline_results['accuracy']:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56716035",
   "metadata": {},
   "source": [
    "## 7. Ï„/Î± Grid Sweep (í•µì‹¬ ì‹¤í—˜)\n",
    "\n",
    "> **ë…¼ë¬¸ ì°¸ê³ **: ASA ë…¼ë¬¸ì—ì„œ Qwen2.5-1.5BëŠ” Î±=4, Ï„=0.60 ì‚¬ìš©.\n",
    "> ìš°ë¦¬ ê¸°ì¡´ ê²°ê³¼ëŠ” Î±=1, Ï„=0.50 â†’ ê³¼ì‰ ì–µì œ ë¬¸ì œ.\n",
    "> Ï„ë¥¼ ë†’ì´ë©´ \"í™•ì‹¤í•  ë•Œë§Œ\" ê°œì…, Î±ë¥¼ í‚¤ìš°ë©´ ê°œì… ì‹œ ë” íš¨ê³¼ì ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb789e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configurations\n",
    "EXPERIMENTS = [\n",
    "    # (name, tau, alpha, beta)\n",
    "    (\"current\",      0.50, 1, 0.0),   # í˜„ì¬ ì„¤ì • (ë¹„êµ ê¸°ì¤€)\n",
    "    (\"paper\",        0.60, 4, 0.0),   # ë…¼ë¬¸ ì„¤ì • ê·¸ëŒ€ë¡œ\n",
    "    (\"gentle\",       0.70, 2, 0.0),   # ë¶€ë“œëŸ¬ìš´ ê°œì…\n",
    "    (\"balanced\",     0.65, 3, 0.0),   # ê· í˜•ì¡íŒ ì„¤ì •\n",
    "    (\"conservative\", 0.70, 4, 0.0),   # ë³´ìˆ˜ì  + ë…¼ë¬¸ Î±\n",
    "    (\"selective\",    0.75, 5, 0.0),   # ë§¤ìš° ì„ íƒì  + ê°•í•œ ì´‰ì§„\n",
    "    (\"paper_high\",   0.60, 8, 0.0),   # ë…¼ë¬¸ Ï„ + ë§¤ìš° ê°•í•œ Î±\n",
    "    (\"minimal\",      0.80, 3, 0.0),   # ìµœì†Œ ê°œì…\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for name, tau, alpha, beta in EXPERIMENTS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Experiment: {name} (Ï„={tau}, Î±={alpha}, Î²={beta})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results = evaluate_split(samples[\"test\"], alpha=alpha, tau=tau, beta=beta, use_asa=True)\n",
    "    all_results[name] = {\"tau\": tau, \"alpha\": alpha, \"beta\": beta, **results}\n",
    "    print(f\"  F1={results['f1']:.4f}  Prec={results['precision']:.4f}  \"\n",
    "          f\"Rec={results['recall']:.4f}  FPR={results['fpr']:.4f}  Acc={results['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\\nAll experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ea467",
   "metadata": {},
   "source": [
    "## 8. Results Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(f\"{'Experiment':>14s} | {'Ï„':>4s} | {'Î±':>3s} | {'F1':>6s} | {'Prec':>6s} | {'Rec':>6s} | {'FPR':>6s} | {'Acc':>6s}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Baseline row\n",
    "print(f\"{'baseline':>14s} | {'--':>4s} | {'--':>3s} | {baseline_results['f1']:>6.4f} | \"\n",
    "      f\"{baseline_results['precision']:>6.4f} | {baseline_results['recall']:>6.4f} | \"\n",
    "      f\"{baseline_results['fpr']:>6.4f} | {baseline_results['accuracy']:>6.4f}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# ASA experiments sorted by F1\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
    "for name, r in sorted_results:\n",
    "    marker = \" â˜…\" if r['f1'] == max(v['f1'] for v in all_results.values()) else \"\"\n",
    "    print(f\"{name:>14s} | {r['tau']:>4.2f} | {r['alpha']:>3d} | {r['f1']:>6.4f} | \"\n",
    "          f\"{r['precision']:>6.4f} | {r['recall']:>6.4f} | {r['fpr']:>6.4f} | \"\n",
    "          f\"{r['accuracy']:>6.4f}{marker}\")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Find best\n",
    "best_f1 = max(all_results.items(), key=lambda x: x[1]['f1'])\n",
    "best_fpr = min(all_results.items(), key=lambda x: x[1]['fpr'])\n",
    "best_recall = max(all_results.items(), key=lambda x: x[1]['recall'])\n",
    "\n",
    "print(f\"\\nâ˜… Best F1:     {best_f1[0]} (Ï„={best_f1[1]['tau']}, Î±={best_f1[1]['alpha']}) â†’ F1={best_f1[1]['f1']:.4f}\")\n",
    "print(f\"â˜… Best FPR:    {best_fpr[0]} (Ï„={best_fpr[1]['tau']}, Î±={best_fpr[1]['alpha']}) â†’ FPR={best_fpr[1]['fpr']:.4f}\")\n",
    "print(f\"â˜… Best Recall: {best_recall[0]} (Ï„={best_recall[1]['tau']}, Î±={best_recall[1]['alpha']}) â†’ Rec={best_recall[1]['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01f9ed",
   "metadata": {},
   "source": [
    "## 9. Comparison with Qwen-style Best (F1=0.6185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7438e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_STYLE_BEST = {\"f1\": 0.6185, \"precision\": 0.7591, \"recall\": 0.5219,\n",
    "                    \"fpr\": 0.1656, \"accuracy\": 0.6781}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'':>14s} | {'F1':>6s} | {'Prec':>6s} | {'Rec':>6s} | {'FPR':>6s} | {'Acc':>6s}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Qwen+ASA':>14s} | {QWEN_STYLE_BEST['f1']:>6.4f} | {QWEN_STYLE_BEST['precision']:>6.4f} | \"\n",
    "      f\"{QWEN_STYLE_BEST['recall']:>6.4f} | {QWEN_STYLE_BEST['fpr']:>6.4f} | {QWEN_STYLE_BEST['accuracy']:>6.4f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, r in sorted_results:\n",
    "    delta_f1 = r['f1'] - QWEN_STYLE_BEST['f1']\n",
    "    delta_fpr = r['fpr'] - QWEN_STYLE_BEST['fpr']\n",
    "    f1_marker = \"â†‘\" if delta_f1 > 0 else \"â†“\"\n",
    "    fpr_marker = \"â†“âœ…\" if delta_fpr < 0 else \"â†‘âŒ\"\n",
    "    print(f\"{'Liq+' + name:>14s} | {r['f1']:>6.4f} | {r['precision']:>6.4f} | \"\n",
    "          f\"{r['recall']:>6.4f} | {r['fpr']:>6.4f} | {r['accuracy']:>6.4f}  \"\n",
    "          f\"Î”F1={delta_f1:+.4f}{f1_marker}  Î”FPR={delta_fpr:+.4f}{fpr_marker}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if any Liquid config beats Qwen-style\n",
    "any_beats = any(r['f1'] > QWEN_STYLE_BEST['f1'] for r in all_results.values())\n",
    "if any_beats:\n",
    "    winner = max(all_results.items(), key=lambda x: x[1]['f1'])\n",
    "    print(f\"\\nğŸ‰ Liquid + {winner[0]} BEATS Qwen-style! F1={winner[1]['f1']:.4f} > {QWEN_STYLE_BEST['f1']:.4f}\")\n",
    "else:\n",
    "    closest = min(all_results.items(), key=lambda x: abs(x[1]['f1'] - QWEN_STYLE_BEST['f1']))\n",
    "    gap = QWEN_STYLE_BEST['f1'] - closest[1]['f1']\n",
    "    print(f\"\\nâš ï¸ No Liquid config beats Qwen-style. Closest: {closest[0]} (gap={gap:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd320f",
   "metadata": {},
   "source": [
    "## 10. Domain-level Analysis for Best Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(all_results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "best = all_results[best_name]\n",
    "print(f\"Best config: {best_name} (Ï„={best['tau']}, Î±={best['alpha']})\\n\")\n",
    "\n",
    "domains = [\"math\", \"code\", \"search\", \"translation\"]\n",
    "print(f\"{'Domain':>12s} | {'F1':>6s} | {'Prec':>6s} | {'Rec':>6s} | {'FPR':>6s}\")\n",
    "print(\"-\" * 55)\n",
    "for dom in domains:\n",
    "    f1 = best.get(f\"{dom}_f1\", 0)\n",
    "    p = best.get(f\"{dom}_precision\", 0)\n",
    "    r = best.get(f\"{dom}_recall\", 0)\n",
    "    fpr = best.get(f\"{dom}_fpr\", 0)\n",
    "    print(f\"{dom:>12s} | {f1:>6.4f} | {p:>6.4f} | {r:>6.4f} | {fpr:>6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10f5fd",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463239bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "names = [n for n, _ in sorted_results]\n",
    "f1s = [r['f1'] for _, r in sorted_results]\n",
    "fprs = [r['fpr'] for _, r in sorted_results]\n",
    "recalls = [r['recall'] for _, r in sorted_results]\n",
    "\n",
    "# F1 bar chart\n",
    "ax = axes[0]\n",
    "colors = ['#2ecc71' if f > QWEN_STYLE_BEST['f1'] else '#e74c3c' if f < 0.4 else '#3498db' for f in f1s]\n",
    "bars = ax.barh(names, f1s, color=colors)\n",
    "ax.axvline(QWEN_STYLE_BEST['f1'], color='orange', linestyle='--', label=f\"Qwen+ASA ({QWEN_STYLE_BEST['f1']:.3f})\")\n",
    "ax.axvline(baseline_results['f1'], color='gray', linestyle=':', label=f\"Baseline ({baseline_results['f1']:.3f})\")\n",
    "ax.set_xlabel('F1 Score')\n",
    "ax.set_title('F1 Score by Config')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# FPR bar chart\n",
    "ax = axes[1]\n",
    "colors = ['#2ecc71' if f < QWEN_STYLE_BEST['fpr'] else '#e74c3c' if f > 0.3 else '#3498db' for f in fprs]\n",
    "bars = ax.barh(names, fprs, color=colors)\n",
    "ax.axvline(QWEN_STYLE_BEST['fpr'], color='orange', linestyle='--', label=f\"Qwen+ASA ({QWEN_STYLE_BEST['fpr']:.3f})\")\n",
    "ax.axvline(baseline_results['fpr'], color='gray', linestyle=':', label=f\"Baseline ({baseline_results['fpr']:.3f})\")\n",
    "ax.set_xlabel('FPR (lower=better)')\n",
    "ax.set_title('False Positive Rate by Config')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlim(0, 0.8)\n",
    "\n",
    "# Recall bar chart\n",
    "ax = axes[2]\n",
    "colors = ['#2ecc71' if r > 0.4 else '#e74c3c' if r < 0.25 else '#3498db' for r in recalls]\n",
    "bars = ax.barh(names, recalls, color=colors)\n",
    "ax.axvline(QWEN_STYLE_BEST['recall'], color='orange', linestyle='--', label=f\"Qwen+ASA ({QWEN_STYLE_BEST['recall']:.3f})\")\n",
    "ax.axvline(baseline_results['recall'], color='gray', linestyle=':', label=f\"Baseline ({baseline_results['recall']:.3f})\")\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_title('Recall by Config')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle('ASA Ï„/Î± Tuning â€” Liquid Prompt on LFM2.5', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs_liquid/tuning_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Chart saved to outputs_liquid/tuning_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2ff8e",
   "metadata": {},
   "source": [
    "## 12. FPR vs Recall Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot all experiments\n",
    "for name, r in all_results.items():\n",
    "    ax.scatter(r['fpr'], r['recall'], s=100, zorder=5)\n",
    "    ax.annotate(f\"{name}\\nÏ„={r['tau']},Î±={r['alpha']}\\nF1={r['f1']:.3f}\",\n",
    "                (r['fpr'], r['recall']), textcoords=\"offset points\",\n",
    "                xytext=(10, 5), fontsize=8)\n",
    "\n",
    "# Reference points\n",
    "ax.scatter(baseline_results['fpr'], baseline_results['recall'], s=200, marker='X',\n",
    "           color='gray', zorder=6, label='Baseline (no ASA)')\n",
    "ax.scatter(QWEN_STYLE_BEST['fpr'], QWEN_STYLE_BEST['recall'], s=200, marker='D',\n",
    "           color='orange', zorder=6, label='Qwen+ASA (best)')\n",
    "\n",
    "# Ideal region\n",
    "ax.axhspan(0.4, 1.0, xmin=0, xmax=0.3, alpha=0.1, color='green', label='Ideal zone')\n",
    "\n",
    "ax.set_xlabel('FPR (lower = better) â†’', fontsize=12)\n",
    "ax.set_ylabel('Recall (higher = better) â†’', fontsize=12)\n",
    "ax.set_title('FPR vs Recall Tradeoff â€” Liquid Prompt ASA Tuning', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(-0.02, 0.75)\n",
    "ax.set_ylim(-0.02, 0.75)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs_liquid/pareto_front.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Pareto chart saved to outputs_liquid/pareto_front.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf31e4",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ace72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"  FINAL SUMMARY: Liquid Prompt Ï„/Î± Tuning\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_name, best = max(all_results.items(), key=lambda x: x[1]['f1'])\n",
    "best_fpr_name, best_fpr_r = min(all_results.items(), key=lambda x: x[1]['fpr'])\n",
    "\n",
    "print(f\"\\n  [Reference] Qwen-style + ASA:\")\n",
    "print(f\"    F1={QWEN_STYLE_BEST['f1']:.4f}  FPR={QWEN_STYLE_BEST['fpr']:.4f}  Rec={QWEN_STYLE_BEST['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n  [Best F1] Liquid + {best_name} (Ï„={best['tau']}, Î±={best['alpha']}):\")\n",
    "print(f\"    F1={best['f1']:.4f}  FPR={best['fpr']:.4f}  Rec={best['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n  [Best FPR] Liquid + {best_fpr_name} (Ï„={best_fpr_r['tau']}, Î±={best_fpr_r['alpha']}):\")\n",
    "print(f\"    F1={best_fpr_r['f1']:.4f}  FPR={best_fpr_r['fpr']:.4f}  Rec={best_fpr_r['recall']:.4f}\")\n",
    "\n",
    "delta_f1 = best['f1'] - QWEN_STYLE_BEST['f1']\n",
    "if delta_f1 > 0:\n",
    "    print(f\"\\n  ğŸ‰ RESULT: Liquid prompt with tuned Ï„/Î± BEATS Qwen-style by +{delta_f1:.4f} F1!\")\n",
    "    print(f\"  â†’ Recommendation: Use Liquid prompt with Ï„={best['tau']}, Î±={best['alpha']}\")\n",
    "elif delta_f1 > -0.05:\n",
    "    print(f\"\\n  âš ï¸ RESULT: Close to Qwen-style (gap={-delta_f1:.4f}) with better FPR.\")\n",
    "    if best['fpr'] < QWEN_STYLE_BEST['fpr']:\n",
    "        print(f\"  â†’ Liquid has lower FPR ({best['fpr']:.4f} vs {QWEN_STYLE_BEST['fpr']:.4f})\")\n",
    "        print(f\"  â†’ Consider Liquid for FPR-sensitive applications\")\n",
    "else:\n",
    "    print(f\"\\n  âŒ RESULT: Qwen-style remains better by {-delta_f1:.4f} F1.\")\n",
    "    print(f\"  â†’ Stick with Qwen-style + ASA for best overall performance\")\n",
    "    if best_fpr_r['fpr'] < QWEN_STYLE_BEST['fpr']:\n",
    "        print(f\"  â†’ But Liquid + {best_fpr_name} is best for FPR-critical scenarios ({best_fpr_r['fpr']:.4f})\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b9f5a",
   "metadata": {},
   "source": [
    "## 14. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "save_data = {\n",
    "    \"baseline\": baseline_results,\n",
    "    \"qwen_style_reference\": QWEN_STYLE_BEST,\n",
    "    \"experiments\": all_results,\n",
    "    \"best_f1_config\": best_name,\n",
    "    \"best_fpr_config\": best_fpr_name,\n",
    "}\n",
    "\n",
    "os.makedirs(\"outputs_liquid\", exist_ok=True)\n",
    "with open(\"outputs_liquid/tuning_results.json\", \"w\") as f:\n",
    "    json.dump(save_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"Results saved to outputs_liquid/tuning_results.json\")\n",
    "print(f\"\\nTotal experiments run: {len(all_results)}\")\n",
    "print(f\"Best F1: {best_name} â†’ {best['f1']:.4f}\")\n",
    "print(f\"Best FPR: {best_fpr_name} â†’ {best_fpr_r['fpr']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
